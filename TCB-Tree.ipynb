{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLQqYq2dF4Ff",
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans,DBSCAN\n",
    "from datetime import datetime, timedelta\n",
    "from collections.abc import Iterable\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tracemalloc\n",
    "import numpy as np\n",
    "import warnings\n",
    "import random\n",
    "import shutil\n",
    "import timeit\n",
    "import psutil\n",
    "import time\n",
    "import pdb\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = psutil.Process(os.getpid())\n",
    "start_mem = process.memory_info().rss / (1024 ** 2)  # in MB\n",
    "tracemalloc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global constants for various settings\n",
    "\n",
    "# CONSUMPTION_RANGES = [\n",
    "#     [0, 14428.0],      # Range 1: from 0 to 14428.0\n",
    "#     [14430.0, 17532.0],# Range 2: from 14430.0 to 17532.0\n",
    "#     [17535.0, float('inf')]  # Range 3: from 17535.0 to infinity\n",
    "# ]\n",
    "\n",
    "CSV_FILE_PATHS=['./dataset/AEP_hourly.csv','./dataset/electricityConsumptionAndProductioction.csv','./dataset/Load Consumption Data Algeria.csv']\n",
    "\n",
    " \n",
    " \n",
    "CSV_FILE_PATH = './dataset/Load Consumption Data Algeria.csv'  # The path where the long-format data will be saved\n",
    "TIME_COLUMN = 'DateTime'  # Column name for datetime data\n",
    "CONSUMPTION_COLUMN = 'Consumption'  # Column name for consumption data\n",
    "COLUMNS_TO_KEEP = [TIME_COLUMN, CONSUMPTION_COLUMN]\n",
    "\n",
    "NUM_SEARCH_COUNT=1\n",
    "\n",
    "\n",
    "DEFAULT_DATE_FORMAT = '%Y-%m-%d %H:%M'  # Default format for date and time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_folder_name = os.path.splitext(os.path.basename(CSV_FILE_PATH))[0]\n",
    "\n",
    "folder_name = \"./output/\"+base_folder_name \n",
    "\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "os.makedirs(folder_name+\"/search intervals\", exist_ok=True)\n",
    "# folder_name = folder_name+\"/data_groups.csv\"\n",
    "\n",
    "# output_file_path = os.path.join(folder_name, 'data_groups.csv')\n",
    "print(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def rename_columns_in_csv(file_path, rename_dict, save_path=None, overwrite=True):\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    df.rename(columns=rename_dict, inplace=True)\n",
    "    print(df.columns)\n",
    "    df=df[COLUMNS_TO_KEEP]\n",
    "    if overwrite:\n",
    "        save_path = file_path  # Overwrite the original file\n",
    "    elif save_path is None:\n",
    "        raise ValueError(\"save_path must be provided if overwrite is False.\")\n",
    "    \n",
    "    df.to_csv(save_path, index=False)\n",
    "    print(f\"CSV file saved to {save_path} successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compare_time_intervals_fixed_step(csv_paths, time_column):\n",
    "    comparison_data = []\n",
    "\n",
    "    for path in csv_paths:\n",
    "        dataset_name = path.split('/')[-1].replace('.csv', '')\n",
    "\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        df[time_column] = pd.to_datetime(df[time_column])\n",
    "\n",
    "        first_time = df[time_column].min()\n",
    "        last_time = df[time_column].max()\n",
    "\n",
    "        total_data_points = df[time_column].count()\n",
    "\n",
    "        comparison_data.append({\n",
    "            'Dataset': dataset_name,\n",
    "            'First Time': first_time,\n",
    "            'Last Time': last_time,\n",
    "            'Total Data Points': total_data_points,\n",
    "            'Time Step': '1h 0m 0s'\n",
    "        })\n",
    "\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "    comparison_df = comparison_df.sort_values(by='Total Data Points', ascending=False)\n",
    "\n",
    "    return comparison_df\n",
    "\n",
    "\n",
    "comparison_table = compare_time_intervals_fixed_step(CSV_FILE_PATHS, TIME_COLUMN)\n",
    "\n",
    "display(comparison_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe_to_csv(dataframe, file_path):\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if directory and not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    dataframe.to_csv(file_path, index=False)\n",
    "    print(f\"DataFrame saved successfully to {file_path}\")\n",
    "\n",
    "file_path = './output/comparison_data_frames.csv'\n",
    "\n",
    "save_dataframe_to_csv(comparison_table, file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu_availability():\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "    if device_name != '/device:GPU:0':\n",
    "        print('\\n\\nError: Configure notebook to use a GPU.')\n",
    "        raise SystemError('GPU device not found')\n",
    "    print('GPU is available and configured.')\n",
    "\n",
    "def check_tensorflow_version_and_gpus():\n",
    "    print(\"TensorFlow version:\", tf.__version__)\n",
    "    print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "def cpu_operation():\n",
    "    \"\"\"Perform a convolution operation on the CPU.\"\"\"\n",
    "    with tf.device('/cpu:0'):\n",
    "        random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
    "        net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
    "        return tf.math.reduce_sum(net_cpu)\n",
    "\n",
    "def gpu_operation():\n",
    "    \"\"\"Perform a convolution operation on the GPU.\"\"\"\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
    "        net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
    "        return tf.math.reduce_sum(net_gpu)\n",
    "\n",
    "def measure_performance():\n",
    "    \"\"\"Measure and print the performance of CPU and GPU operations.\"\"\"\n",
    "    print('CPU (s): ' + str(timeit.timeit('cpu_operation()', number=10, setup=\"from __main__ import cpu_operation\")))\n",
    "    print('GPU (s): ' + str(timeit.timeit('gpu_operation()', number=10, setup=\"from __main__ import gpu_operation\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution\n",
    "try:\n",
    "    check_gpu_availability()\n",
    "    check_tensorflow_version_and_gpus()\n",
    "    measure_performance()\n",
    "except SystemError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    # Handle the exception or provide instructions to the user\n",
    "    print(\"Please configure your environment to use a GPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68J630HoVca5",
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "outputId": "9798f2f3-277a-4f7e-d94c-3633480f4a2e"
   },
   "outputs": [],
   "source": [
    "def read_and_sort_csv_by_time(file_path, time_column):\n",
    "    \"\"\"Read a CSV file and sort it by the specified time column.\"\"\"\n",
    "    \n",
    "    # Read the CSV file into a DataFrame\n",
    "    all_df = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Ensure the time column is in datetime format\n",
    "    df[time_column] = pd.to_datetime(df[time_column])\n",
    "    duplicates = df[df.duplicated(subset=time_column, keep=False)]\n",
    "    df_unique = df.drop_duplicates(subset=time_column, keep='first')\n",
    "\n",
    "    # Sort the DataFrame by the time column\n",
    "    df_sorted = df.sort_values(by=time_column)\n",
    "    \n",
    "    return df_sorted\n",
    "\n",
    "def rename_columns(df, consumption_column='AEP_MW', time_column='Datetime', new_consumption_name=CONSUMPTION_COLUMN, new_time_name=TIME_COLUMN):\n",
    "    # Renames specified columns in the DataFrame.\n",
    "    if consumption_column in df.columns:\n",
    "        df.rename(columns={consumption_column: new_consumption_name}, inplace=True)\n",
    "    if time_column in df.columns:\n",
    "        df.rename(columns={time_column: new_time_name}, inplace=True)\n",
    "    return df\n",
    "\n",
    "def filter_dataframe(df, fraction=10):\n",
    "    df = df.copy()\n",
    "    df=df[:len(df)//10]\n",
    "    return df\n",
    "\n",
    "def keep_columns(df, columns_to_keep):\n",
    "    df = df[columns_to_keep]\n",
    "    return df\n",
    "\n",
    "def format_date_column(df):\n",
    "    df[TIME_COLUMN] = pd.to_datetime(df[TIME_COLUMN])\n",
    "    df[TIME_COLUMN] = df[TIME_COLUMN].dt.strftime('%Y-%m-%d %H:%M')\n",
    "    return df\n",
    "\n",
    "def scale_consumption_data(df):\n",
    "    # Reshape CONSUMPTION_COLUMN data\n",
    "    X = df[CONSUMPTION_COLUMN].values.reshape(-1, 1)\n",
    "    \n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    return X_scaled, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_df = read_and_sort_csv_by_time(CSV_FILE_PATH, TIME_COLUMN)\n",
    "all_df = all_df.reset_index(drop=True)\n",
    "\n",
    "# print(all_df.head())\n",
    "\n",
    "# all_df = rename_columns(all_df, CONSUMPTION_COLUMN, TIME_COLUMN)  # Initial DataFrame setup\n",
    "\n",
    "all_df = format_date_column(all_df)  # Format date column\n",
    "\n",
    "df = filter_dataframe(all_df,5)  # Filter DataFrame to 10% of original\n",
    "\n",
    "# df = keep_columns(df, COLUMNS_TO_KEEP)  # Keep specific columns\n",
    "\n",
    "X_scaled, scaler = scale_consumption_data(df)  # Apply scaling to the 'consumption' data\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def transform_wide_to_long(input_file: str, output_file: str) -> pd.DataFrame:\n",
    "    # Read the wide-format Excel file\n",
    "    df = pd.read_excel(input_file)\n",
    "    \n",
    "    rename_dict = {f'{i+1}h': f'{i}h' for i in range(25)}\n",
    "    df.rename(columns=rename_dict, inplace=True)\n",
    "    # df = df.rename(columns={\"24h\": \"0h\"})\n",
    "    # Melt the DataFrame from wide to long format\n",
    "    # print(df.columns)\n",
    "    df_long = df.melt(id_vars=['Date'], var_name='Hour', value_name=CONSUMPTION_COLUMN)\n",
    "\n",
    "    # Convert Date column to string\n",
    "    df_long['Date'] = df_long['Date'].astype(str)\n",
    "\n",
    "    # Convert Hour to the proper time format\n",
    "    df_long['Hour'] = df_long['Hour'].str.replace('h', ':00')\n",
    "\n",
    "    # Combine Date and Hour into \"date and time\", infer format if necessary\n",
    "    df_long[TIME_COLUMN] = pd.to_datetime(df_long['Date'] + ' ' + df_long['Hour'], errors='coerce')\n",
    "\n",
    "    # Drop rows with invalid dates (if any)\n",
    "    df_long = df_long.dropna(subset=[TIME_COLUMN])\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    df_long = df_long[[TIME_COLUMN, CONSUMPTION_COLUMN]]\n",
    "\n",
    "    # Sort by date and time\n",
    "    df_long = df_long.sort_values(by=TIME_COLUMN)\n",
    "\n",
    "    # Reset index\n",
    "    df_long = df_long.reset_index(drop=True)\n",
    "\n",
    "    # Save the long-format DataFrame to a new CSV file\n",
    "    df_long.to_csv(output_file, index=False)\n",
    "\n",
    "    return df_long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Example usage\n",
    "# file_path = './Labstic/BDD_E.xlsx'  # Replace with your actual file path\n",
    "# output_path = './Labstic/converted_dataset.csv'  # The path where the long-format data will be saved\n",
    "# transformed_df = transform_wide_to_long(file_path, output_path)\n",
    "# display(transformed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = 'Times New Roman'\n",
    "\n",
    "\n",
    "\n",
    "def calc_consumption_ranges(df, X_scaled, k=3):\n",
    "    df['cluster'] = KMeans(n_clusters=k, random_state=42).fit_predict(X_scaled)\n",
    "    consumption_ranges = df.groupby('cluster')[CONSUMPTION_COLUMN].agg(['min', 'max']).sort_values('min').values.tolist()\n",
    "    consumption_ranges[0][0], consumption_ranges[-1][1] = 0, float('inf')\n",
    "    unique_consumption_levels = [0] + [r[1] for r in consumption_ranges[:-1]]\n",
    "    return consumption_ranges, unique_consumption_levels\n",
    "def plot_clusters(df, X_scaled, k=3, filename='clusters_plot.pdf'):\n",
    "    # Perform clustering using the method already defined\n",
    "    calc_consumption_ranges(df, X_scaled, k)\n",
    "    \n",
    "    # Define the more beautiful colors for the clusters\n",
    "    cluster_colors = ['#4CAF50', '#FFB74D', '#81D4FA']  # Beautiful colors in the same family\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))  # Set figure size to 20 width by 10 height\n",
    "\n",
    "    # Initialize variables to determine y-axis limits\n",
    "    min_y = float('inf')\n",
    "    max_y = float('-inf')\n",
    "\n",
    "    # Plot the clusters with different colors\n",
    "    for cluster in range(k):\n",
    "        cluster_data = df[df['cluster'] == cluster]\n",
    "        ax.scatter(cluster_data.index, cluster_data[CONSUMPTION_COLUMN], label=f'Cluster {cluster}', \n",
    "                   color=cluster_colors[cluster], alpha=0.6, edgecolors='black', s=50)\n",
    "\n",
    "        # Update y-axis limits\n",
    "        min_y = min(min_y, min(cluster_data[CONSUMPTION_COLUMN]))\n",
    "        max_y = max(max_y, max(cluster_data[CONSUMPTION_COLUMN]))\n",
    "\n",
    "    # Add labels, title, and legend\n",
    "    ax.set_title('Clusters of Consumption Data', fontsize=20, pad=20)\n",
    "    ax.set_xlabel('DateTime', fontsize=20, labelpad=20)\n",
    "    ax.set_ylabel(CONSUMPTION_COLUMN, fontsize=20, labelpad=20)\n",
    "\n",
    "    ax.tick_params(axis='both', which='major', labelsize=20)  # Adjust the size of the tick labels\n",
    "    ax.tick_params(axis='both', which='both', labelcolor='black')  # Optional: Customize tick label color\n",
    "    ax.legend(fontsize=15, loc='upper left')  # Add legend with adjusted font size\n",
    "\n",
    "    # Set y-axis limits to fit the data\n",
    "    buffer = (max_y - min_y) * 0.1  # Add some buffer space\n",
    "    ax.set_ylim(min_y - buffer, max_y + buffer)\n",
    "\n",
    "    # Add grid to the plot\n",
    "    ax.grid(True, which='both', linestyle='--', linewidth=0.7)  # Grid with dashed lines\n",
    "\n",
    "    # Set the x-ticks to correspond to time stamps\n",
    "    # Convert 'DateTime' to datetime if it's not already\n",
    "    ax.set_xticks(df.index[::int(len(df) / 10)])  # Choose a subset of x-ticks, adjust as needed\n",
    "    ax.set_xticklabels(pd.to_datetime(df['DateTime']).iloc[::int(len(df) / 10)].dt.strftime('%Y-%m-%d %H:%M:%S'), rotation=45, ha='right')\n",
    "    ax.grid(False)\n",
    "\n",
    "    # Save the plot as a vectorial PDF\n",
    "    plt.savefig(filename, format='pdf', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSUMPTION_RANGES, LEVEL_SEPARATORS = calc_consumption_ranges(df, X_scaled)\n",
    "print(CONSUMPTION_RANGES)\n",
    "print(LEVEL_SEPARATORS)\n",
    "\n",
    "plot_clusters(df, X_scaled, k=3, filename='clusters_plot.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ranges_and_separators_to_csv(consumption_ranges, level_separators, input_csv_path):\n",
    "    # Construct the output directory and file path\n",
    "    output_dir = f\"./output/{os.path.splitext(os.path.basename(input_csv_path))[0]}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file_path = os.path.join(output_dir, 'consumption_ranges_and_separators.csv')\n",
    "\n",
    "    # Prepare data for saving\n",
    "    consumption_ranges_df = pd.DataFrame({\"CONSUMPTION_RANGES\": [str(r) for r in consumption_ranges]})\n",
    "    level_separators_df = pd.DataFrame({\"LEVEL_SEPARATORS\": level_separators})\n",
    "    \n",
    "    # Concatenate the two DataFrames, aligning them vertically\n",
    "    combined_df = pd.concat([consumption_ranges_df, level_separators_df], axis=1)\n",
    "    \n",
    "    # Save to CSV\n",
    "    combined_df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Ranges and separators saved successfully to {output_file_path}\")\n",
    "\n",
    "\n",
    "# Save the data\n",
    "save_ranges_and_separators_to_csv(CONSUMPTION_RANGES, LEVEL_SEPARATORS, CSV_FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-pBUAB2u7e-"
   },
   "outputs": [],
   "source": [
    "class TreeNode():\n",
    "    def __init__(self):\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.key = None\n",
    "\n",
    "class LeafNode():\n",
    "    def __init__(self):\n",
    "        # super().__init__()\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.key = None\n",
    "        self.data = []\n",
    "        self.level = None\n",
    "        self.timeRange = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XCktCHaMM_ia"
   },
   "outputs": [],
   "source": [
    "def Insert (c,pt,father):\n",
    "  global p,root\n",
    "  if not c:\n",
    "    print(\"empty\")\n",
    "  else:\n",
    "    if p is None :\n",
    "      p = LeafNode()\n",
    "      p.data=c\n",
    "      p.timeRange=[c[0][1],c[len(c)-1][1]]\n",
    "      p.key=c[0][1]\n",
    "      p.level=level(c)\n",
    "\n",
    "    else:\n",
    "      if pt.level > level(c) :\n",
    "        if pt.right is None:\n",
    "          pt.right = LeafNode()\n",
    "          pt.right.timeRange=[c[0][1],c[len(c)-1][1]]\n",
    "          pt.right.key=c[0][1]\n",
    "          pt.right.data=c\n",
    "          pt.right.level=level(c)\n",
    "\n",
    "        else:\n",
    "          Insert(c,pt.right,True)\n",
    "      elif pt.level < level(c) :\n",
    "        # pdb.set_trace()\n",
    "        if father == False:\n",
    "          t = LeafNode()\n",
    "          t.timeRange=[c[0][1],c[len(c)-1][1]]\n",
    "          t.key=c[0][1]\n",
    "          t.data=c\n",
    "          t.level=level(c)\n",
    "          t.left = p\n",
    "          p = t\n",
    "        else:\n",
    "          t = TreeNode()\n",
    "          t.key= get_first_time(p)\n",
    "          t.left = root\n",
    "          t.right = p\n",
    "          root = t\n",
    "\n",
    "          p=LeafNode()\n",
    "          p.timeRange=[c[0][1],c[len(c)-1][1]]\n",
    "          p.key=c[0][1]\n",
    "          p.data=c\n",
    "          p.level=level(c)\n",
    "      elif pt.level == level(c) :  # 000\n",
    "        t = TreeNode()\n",
    "        t.key= get_first_time(p)\n",
    "        t.left = root\n",
    "        t.right = p\n",
    "        root = t\n",
    "\n",
    "        p=LeafNode()\n",
    "        p.timeRange=[c[0][1],c[len(c)-1][1]]\n",
    "        p.key=c[0][1]\n",
    "        p.data=c\n",
    "        p.level=level(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQEZLixeD-KM"
   },
   "outputs": [],
   "source": [
    "def get_total_size(obj, seen=None):  # Recursively find the total size of an object including its members. \n",
    "    size = sys.getsizeof(obj)\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "\n",
    "    obj_id = id(obj)\n",
    "    if obj_id in seen:\n",
    "        return 0\n",
    "\n",
    "    # Mark this object as seen\n",
    "    seen.add(obj_id)\n",
    "\n",
    "    if isinstance(obj, dict):\n",
    "        size += sum([get_total_size(v, seen) for v in obj.values()])\n",
    "        size += sum([get_total_size(k, seen) for k in obj.keys()])\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        size += get_total_size(obj.__dict__, seen)\n",
    "    elif isinstance(obj, Iterable) and not isinstance(obj, (str, bytes, bytearray)):\n",
    "        size += sum([get_total_size(i, seen) for i in obj])\n",
    "\n",
    "    return size\n",
    "\n",
    "def get_first_time(tree):\n",
    "  if tree.left is  None:\n",
    "    return tree.data[0][1]\n",
    "  else: return get_first_time(tree.left)\n",
    "\n",
    "def get_last_time(tree):\n",
    "  if tree.right is None:\n",
    "    return tree.data[len(tree.data)-1][1]\n",
    "  else: return get_first_time(tree.right)\n",
    "\n",
    "def time_range(tree):\n",
    "  return (get_first_time(tree),get_last_time(tree))\n",
    "\n",
    "def allData(tree):\n",
    "  dataListe=[]\n",
    "  if tree.left is not None:\n",
    "    dataListe+=allData(tree.left)\n",
    "  if isinstance(tree,LeafNode):\n",
    "    dataListe+=tree.data\n",
    "  if tree.right is not None:\n",
    "    dataListe+=allData(tree.right)\n",
    "  return(dataListe)\n",
    "\n",
    "def get_consumption_level(input_value, CONSUMPTION_RANGES):\n",
    "  for level, (start, end) in enumerate(CONSUMPTION_RANGES): # Iterate through consumption ranges to find the level\n",
    "    if start <= input_value <= end:\n",
    "      return level+1\n",
    "  return -1      # If input doesn't match any level, return -1 or handle accordingly\n",
    "\n",
    "def level(data):\n",
    "  if data is not None:\n",
    "#     print(data)\n",
    "    return get_consumption_level(data[0][0],CONSUMPTION_RANGES)\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "def add_to_cluster(input_value,date_time,consumptionLevel):\n",
    "  if consumptionLevel <= len(CONSUMPTION_RANGES):    # Check if the input value falls within the specified consumption level\n",
    "    start, end = CONSUMPTION_RANGES[consumptionLevel-1]\n",
    "    if start <= input_value <= end:\n",
    "      cluster.append((input_value,date_time) )     # If it does, add it to the last cluster with the current system time\n",
    "      return True\n",
    "  return False\n",
    "\n",
    "timeRange = lambda cluster: (min((time for _, time in cluster), default=None), max((time for _, time in cluster), default=None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "goX6vhIDKmZ-"
   },
   "outputs": [],
   "source": [
    "def countDataPointInTree(tree):\n",
    "  cnt=0 # Initialize the count variable\n",
    "  if isinstance(tree, TreeNode):\n",
    "    if tree.left is not None:  # Recursively count data points in the left subtree if exist\n",
    "      cnt+=countDataPointInTree (tree.left)\n",
    "    if tree.right is not None:  # Recursively count data points in the right subtree if exist\n",
    "      cnt+=countDataPointInTree (tree.right)\n",
    "  else:\n",
    "    cnt+= len(tree.data) # Count the number of data points in the node\n",
    "    if tree.left is not None:\n",
    "      cnt+=countDataPointInTree (tree.left)\n",
    "    if tree.right is not None:\n",
    "      cnt+=countDataPointInTree (tree.right)\n",
    "  return cnt\n",
    "\n",
    "def countTotalNode(tree):\n",
    "  cnt=1 # Initialize the count variable\n",
    "  if tree.left is not None:\n",
    "    cnt+=countTotalNode (tree.left)\n",
    "  if tree.right is not None:\n",
    "    cnt+=countTotalNode (tree.right)\n",
    "  return cnt\n",
    "\n",
    "def countArtificialNode(tree):\n",
    "  cnt=0 # Initialize the count variable\n",
    "  if isinstance(tree, TreeNode):\n",
    "    cnt+=1\n",
    "    if tree.left is not None:\n",
    "      cnt+=countArtificialNode (tree.left)\n",
    "    if tree.right is not None:\n",
    "      cnt+=countArtificialNode (tree.right)\n",
    "  elif isinstance(tree, LeafNode):\n",
    "    if tree.left is not None:\n",
    "      cnt +=countArtificialNode(tree.left)\n",
    "    if tree.right is not None:\n",
    "      cnt +=countArtificialNode(tree.right)\n",
    "  return cnt\n",
    "\n",
    "def countRealNode(tree):\n",
    "  cnt=0 # Initialize the count variable\n",
    "  if isinstance(tree, TreeNode):\n",
    "    if tree.left is not None:\n",
    "      cnt+=countRealNode (tree.left)\n",
    "    if tree.right is not None:\n",
    "      cnt+=countRealNode (tree.right)\n",
    "  elif isinstance(tree, LeafNode):\n",
    "    cnt+=1\n",
    "    if tree.left is not None:\n",
    "      cnt +=countRealNode(tree.left)\n",
    "    if tree.right is not None:\n",
    "      cnt +=countRealNode(tree.right)\n",
    "  return cnt\n",
    "\n",
    "def countClustersInLeaf(tree):\n",
    "  def countInLeaf(tree):\n",
    "    cnt=1\n",
    "    if tree.left is not None:\n",
    "      cnt+=countInLeaf (tree.left)\n",
    "    if tree.right is not None:\n",
    "      cnt+=countInLeaf (tree.right)\n",
    "    return cnt\n",
    "  dataInLeafs=[]\n",
    "  if isinstance(tree, TreeNode):\n",
    "    if tree.left is not None: # Recursively count clusters in the left subtree if exist\n",
    "      if isinstance(tree.left, LeafNode):\n",
    "        dataInLeafs.append(countInLeaf(tree.left))\n",
    "      else:\n",
    "        dataInLeafs=dataInLeafs+countClustersInLeaf (tree.left)\n",
    "    if tree.right is not None:  # Recursively count clusters in the right subtree if exist\n",
    "      if isinstance(tree.right, LeafNode):\n",
    "        dataInLeafs.append(countInLeaf(tree.right))\n",
    "      else:\n",
    "        dataInLeafs=dataInLeafs+countClustersInLeaf(tree.right)\n",
    "  return dataInLeafs\n",
    "\n",
    "def displayAllData(tree):\n",
    "  cnt=0 # Initialize the count variable\n",
    "  if isinstance(tree, TreeNode):\n",
    "    if tree.left is not None:  # Recursively count data points in the left subtree if exist\n",
    "      displayAllData (tree.left)\n",
    "    if tree.right is not None:  # Recursively count data points in the right subtree if exist\n",
    "      displayAllData (tree.right)\n",
    "  else:\n",
    "    # Add the count to the total count\n",
    "    if tree.left is not None:\n",
    "      displayAllData (tree.left)\n",
    "    print(tree.data) # Count the number of data points in the node\n",
    "    if tree.right is not None:\n",
    "      displayAllData (tree.right)\n",
    "\n",
    "def sizeof_tree(node):\n",
    "    totalsite= 0\n",
    "    if node is None:\n",
    "        return 0\n",
    "    elif isinstance(node,LeafNode):\n",
    "      totalsite+= sys.getsizeof(node) + sizeof_tree(node.left) + sizeof_tree(node.right) +sys.getsizeof(node.key)+sys.getsizeof(node.data)+sys.getsizeof(node.timeRange)+sys.getsizeof(node.level)\n",
    "    elif isinstance(node,TreeNode):\n",
    "      totalsite+= sys.getsizeof(node) + sizeof_tree(node.left) + sizeof_tree(node.right) +sys.getsizeof(node.key)\n",
    "    return totalsite\n",
    "\n",
    "def countNodesInConsumptionLevel(tree,n):\n",
    "  dataInLeafs=[]\n",
    "  if isinstance(tree, TreeNode):\n",
    "    if tree.left is not None: # Recursively count clusters in the left subtree if exist\n",
    "      dataInLeafs=dataInLeafs+countNodesInConsumptionLevel (tree.left,n)\n",
    "    if tree.right is not None:  # Recursively count clusters in the right subtree if exist\n",
    "      dataInLeafs=dataInLeafs+countNodesInConsumptionLevel(tree.right,n)\n",
    "  else:\n",
    "    if tree.level==n:\n",
    "      dataInLeafs.append(len(tree.data))\n",
    "    if tree.left is not None: # Recursively count clusters in the left subtree if exist\n",
    "      dataInLeafs=dataInLeafs+countNodesInConsumptionLevel (tree.left,n)\n",
    "    if tree.right is not None:  # Recursively count clusters in the right subtree if exist\n",
    "      dataInLeafs=dataInLeafs+countNodesInConsumptionLevel(tree.right,n)\n",
    "  return dataInLeafs\n",
    "\n",
    "def display_node_distribution(root):\n",
    "    cntDataBotLevelTree=countClustersInLeaf(root)\n",
    "    counter = Counter(cntDataBotLevelTree)\n",
    "    total_nodes = sum(key * value for key, value in counter.items())\n",
    "\n",
    "    # Convert Counter to DataFrame\n",
    "    df = pd.DataFrame.from_dict(counter, orient='index', columns=['Node Count'])\n",
    "    df.index.name = 'Level'\n",
    "    \n",
    "    # Sort the DataFrame by level\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    # Display the table\n",
    "    print(\"Table of Node Distribution:\")\n",
    "    print(df)\n",
    "    \n",
    "    # Plot the distribution for all levels\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(df.index, df['Node Count'], color='skyblue')\n",
    "    plt.title('Distribution of Cluster Counts in Bot Level Trees')\n",
    "    plt.xlabel('Number of Nodes in Bot Level Tree')\n",
    "    plt.ylabel('Occurrences')\n",
    "    plt.xticks(df.index)\n",
    "    plt.show()\n",
    "    \n",
    "    # Display the total filled nodes\n",
    "    print(f\"\\nTotal filled nodes: {total_nodes}\")\n",
    "\n",
    "def process_consumption_levels(root, levels):\n",
    "    result_dict = {}\n",
    "    dictionnair = []\n",
    "    \n",
    "    for i in range(1,len(CONSUMPTION_RANGES)+1):\n",
    "        values = countNodesInConsumptionLevel(root, i)  # Assuming this function is defined elsewhere\n",
    "        count_values = Counter(values)\n",
    "        \n",
    "        print(\"\\n Consumption level %s :\" % i)\n",
    "        dictionnair.append(dict(count_values))\n",
    "        print(dictionnair[i-1])\n",
    "        \n",
    "        # Sum of all data points in this consumption level\n",
    "        total_data_points = sum(key * value for key, value in count_values.items())\n",
    "        print(\"Number of DataPoints in Consumption level %s : %s\" % (i, total_data_points))\n",
    "        \n",
    "        # Sum of all nodes in this consumption level\n",
    "        total_nodes = sum(count_values.values())\n",
    "        print(\"Number of Nodes in Consumption level %s : %s\" % (i, total_nodes))\n",
    "        \n",
    "        # Add results to the overall result dictionary\n",
    "        for key, value in count_values.items():\n",
    "            result_dict[key] = result_dict.get(key, 0) + value\n",
    "    \n",
    "    # Store and print the aggregated result for all data\n",
    "    final_dict = dict(sorted(result_dict.items()))\n",
    "    dictionnair.append(final_dict)\n",
    "    print(\"\\n  For all data: %s\" % final_dict)\n",
    "    \n",
    "    return dictionnair\n",
    "\n",
    "def plot_schematic(dictionnair):\n",
    "    levels = len(dictionnair)\n",
    "    all_keys = sorted(set().union(*(d.keys() for d in dictionnair)))\n",
    "    \n",
    "    # Create a matrix where rows correspond to levels and columns to keys\n",
    "    data_matrix = np.zeros((levels, len(all_keys)))\n",
    "    key_to_index = {key: idx for idx, key in enumerate(all_keys)}\n",
    "    \n",
    "    for i, data in enumerate(dictionnair):\n",
    "        for key, value in data.items():\n",
    "            data_matrix[i, key_to_index[key]] = value\n",
    "    \n",
    "    # Adjust figure size to be very wide\n",
    "    fig, ax = plt.subplots(figsize=(25, 10))\n",
    "    bar_width = 0.8 / levels\n",
    "    index = np.arange(len(all_keys))\n",
    "    \n",
    "    # Plot bars for each level side by side\n",
    "    for i in range(levels):\n",
    "        ax.bar(index + i * bar_width, data_matrix[i], bar_width,\n",
    "               color=plt.get_cmap('tab10').colors[i % len(plt.get_cmap('tab10').colors)],\n",
    "               label=f'Level {i + 1}')\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_xlabel('Data Point Length', fontsize=25)\n",
    "    ax.set_ylabel('Number of Tree Nodes', fontsize=25)\n",
    "    ax.set_title('Schematic of Consumption Levels', fontsize=30)\n",
    "    ax.set_xticks(index + bar_width * (levels / 2))\n",
    "    ax.set_xticklabels(all_keys, rotation=45, ha='right')\n",
    "    ax.legend(fontsize=25)\n",
    "    ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_schematic_ignore_small_values(dictionnair, threshold=50):\n",
    "    levels = len(dictionnair)\n",
    "    all_keys = sorted(set().union(*(d.keys() for d in dictionnair)))\n",
    "    \n",
    "    # Create a matrix where rows correspond to levels and columns to keys\n",
    "    data_matrix = np.zeros((levels, len(all_keys)))\n",
    "    key_to_index = {key: idx for idx, key in enumerate(all_keys)}\n",
    "    \n",
    "    for i, data in enumerate(dictionnair):\n",
    "        for key, value in data.items():\n",
    "            data_matrix[i, key_to_index[key]] = value\n",
    "    \n",
    "    # Calculate the total number of tree nodes for each key\n",
    "    total_nodes = np.sum(data_matrix, axis=0)\n",
    "    \n",
    "    # Filter out keys where the total is less than the threshold\n",
    "    valid_keys = [key for key, total in zip(all_keys, total_nodes) if total >= threshold]\n",
    "    valid_indices = [key_to_index[key] for key in valid_keys]\n",
    "    \n",
    "    # Filter the data matrix to only include valid keys\n",
    "    filtered_data_matrix = data_matrix[:, valid_indices]\n",
    "    \n",
    "    # Adjust figure size to be very wide\n",
    "    fig, ax = plt.subplots(figsize=(25, 10))\n",
    "    bar_width = 0.8 / levels\n",
    "    index = np.arange(len(valid_keys))\n",
    "    \n",
    "    # Plot bars for each level side by side\n",
    "    for i in range(levels):\n",
    "        ax.bar(index + i * bar_width, filtered_data_matrix[i], bar_width,\n",
    "               color=plt.get_cmap('tab10').colors[i % len(plt.get_cmap('tab10').colors)],\n",
    "               label=f'Level {i + 1}')\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_xlabel('Data Point Length', fontsize=25)\n",
    "    ax.set_ylabel('Number of Tree Nodes', fontsize=25)\n",
    "    ax.set_title(f'Schematic of Consumption Levels (Threshold: {threshold})', fontsize=30)\n",
    "    ax.set_xticks(index + bar_width * (levels / 2))\n",
    "    ax.set_xticklabels(valid_keys, rotation=45, ha='right', fontsize=25)\n",
    "    ax.legend(fontsize=25)\n",
    "    ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_range_index(consumption):\n",
    "    for i, (start, end) in range_dict.items():\n",
    "        if start <= consumption <= end:\n",
    "            return i\n",
    "\n",
    "def group_consumption_ranges(df, CONSUMPTION_RANGES):\n",
    "\n",
    "    for i, (start, end) in enumerate(CONSUMPTION_RANGES):\n",
    "        range_dict[i] = (start, end)\n",
    "\n",
    "\n",
    "    df['range_index'] = df[CONSUMPTION_COLUMN].apply(get_range_index)\n",
    "\n",
    "    groups = []\n",
    "    current_group = []\n",
    "    prev_index = None\n",
    "    for index, row in df.iterrows():\n",
    "        if prev_index is None:\n",
    "            current_group.append((row[CONSUMPTION_COLUMN], row[TIME_COLUMN]))\n",
    "        elif row['range_index'] == prev_index:\n",
    "            current_group.append((row[CONSUMPTION_COLUMN], row[TIME_COLUMN]))\n",
    "        else:\n",
    "            groups.append(current_group)\n",
    "            current_group = [(row[CONSUMPTION_COLUMN], row[TIME_COLUMN])]\n",
    "        prev_index = row['range_index']\n",
    "\n",
    "    if current_group:\n",
    "        groups.append(current_group)\n",
    "\n",
    "    return groups\n",
    "\n",
    "def filter_groups_by_length(groups, n):\n",
    "  filtered_groups = [group for group in groups if len(group) >= n]\n",
    "  return filtered_groups\n",
    "\n",
    "def merge_groups_within_ranges(filtered_groups, consumption_ranges, get_range_index):\n",
    "    merged_groups = []\n",
    "    merged_groups.append(filtered_groups[0])  # Initialize merged_groups with the first group\n",
    "    for group in filtered_groups[1:]:  # Iterate through the rest of the filtered_groups\n",
    "        current_range_index = get_range_index(group[0][0])  # Get the consumption range index for the current group\n",
    "        last_merged_range_index = get_range_index(merged_groups[-1][-1][0])  # Get the consumption range index for the last group in merged_groups\n",
    "        if current_range_index == last_merged_range_index:  # If the current group and the last group in merged_groups have the same range index\n",
    "            merged_groups[-1].extend(group)  # Merge the current group with the last group in merged_groups\n",
    "        else:\n",
    "            merged_groups.append(group)  # Append the current group to merged_groups\n",
    "    return merged_groups\n",
    "\n",
    "def process_data_in_chunks(dataframe, chunk_size, consumption_ranges, n, get_range_index):\n",
    "    final_result_data_groups = []\n",
    "    number_of_data_inserted = 0\n",
    "    \n",
    "    # Traverse the data in chunks of the given chunk_size\n",
    "    for i in range(0, len(dataframe), chunk_size):\n",
    "        # Process the current chunk\n",
    "        df_chunk = dataframe[i:i + chunk_size]\n",
    "        \n",
    "        # Group the data based on consumption ranges\n",
    "        result_groups = group_consumption_ranges(df_chunk, consumption_ranges)\n",
    "        \n",
    "        # Filter groups by their length\n",
    "        result_filtered_groups = filter_groups_by_length(result_groups, n)\n",
    "        \n",
    "        # Merge groups within the consumption ranges\n",
    "        result_merged_groups = merge_groups_within_ranges(result_filtered_groups, consumption_ranges, get_range_index)\n",
    "        \n",
    "        # Append each merged group to the final result and count the data points\n",
    "        for group in result_merged_groups:\n",
    "            final_result_data_groups.append(group)\n",
    "            number_of_data_inserted += len(group)\n",
    "    \n",
    "    return final_result_data_groups, number_of_data_inserted\n",
    "\n",
    "def convert_to_datetime(timestamp):\n",
    "    return datetime.strptime(timestamp, '%Y-%m-%d %H:%M')\n",
    "\n",
    "def plot_group(group, group_index, ax):\n",
    "    values = [point[0] for point in group]\n",
    "    timestamps = [convert_to_datetime(point[1]) for point in group]\n",
    "    ax.plot(timestamps, values, label=f'Group {group_index + 1}')\n",
    "\n",
    "def plot_consumption_groups(final_result_data_groups, level_separators, figsize=(20, 8), max_groups=88):\n",
    "    # Function to convert string timestamps to datetime objects\n",
    "    \n",
    "    # Create a plot for the groups\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Plot each group\n",
    "    for i, group in enumerate(final_result_data_groups[:max_groups]):\n",
    "        plot_group(group, i, ax)\n",
    "    \n",
    "    # Add horizontal lines for consumption levels\n",
    "    for row in level_separators:\n",
    "        ax.axhline(y=row, color='blue', linestyle='--', linewidth=1)\n",
    "\n",
    "    # Add labels and titles\n",
    "    plt.xlabel('Timestamp')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Time Series Data for Multiple Groups')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_first_n_groups_with_ranges_croped_pic(groups, consumption_ranges, n=20):\n",
    "    # Define a color map with a distinct color for each group\n",
    "    colors = plt.cm.get_cmap('tab20', n)  # Use 'tab20' colormap with n colors\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20, 8))  # Set figure size to 20 width by 5 height\n",
    "\n",
    "    # Initialize variables to determine y-axis limits\n",
    "    min_y = float('inf')\n",
    "    max_y = float('-inf')\n",
    "\n",
    "    # Plot the first n groups\n",
    "    for i, group in enumerate(groups[:n]):\n",
    "        group_indices = range(sum(len(g) for g in groups[:i]), sum(len(g) for g in groups[:i + 1]))\n",
    "        group_consumptions = [item[0] for item in group]\n",
    "        \n",
    "        # Update y-axis limits\n",
    "        min_y = min(min_y, min(group_consumptions))\n",
    "        max_y = max(max_y, max(group_consumptions))\n",
    "        \n",
    "        # Plot each group with a line and circles for data points\n",
    "        ax.plot(group_indices, group_consumptions, color=colors(i), linestyle='-')\n",
    "        ax.scatter(group_indices, group_consumptions, s=50, edgecolors='black', facecolors='none', marker='o', color=colors(i))\n",
    "\n",
    "    # Set y-axis limits to fit the data\n",
    "    buffer = (max_y - min_y) * 0.1  # Add some buffer space\n",
    "    ax.set_ylim(min_y - buffer, max_y + buffer)\n",
    "\n",
    "    # Plot horizontal lines for the consumption ranges\n",
    "    for start, end in CONSUMPTION_RANGES:\n",
    "        ax.axhline(y=start, color='r', linestyle='--')  # Lower bound of range\n",
    "        ax.axhline(y=end, color='r', linestyle='--')    # Upper bound of range\n",
    "\n",
    "    ax.set_title('Time Series Data Grouped by Consumption Ranges with Range Lines')\n",
    "    ax.set_xlabel('Time Step')\n",
    "    ax.set_ylabel('Consumption')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = value\n",
    "\n",
    "dataframe=all_df.copy()\n",
    "chunk_size = chunk_size  # Define the chunk size\n",
    "number_of_data_inserted=0\n",
    "range_dict={}\n",
    "\n",
    "\n",
    "\n",
    "final_result_data_groups, total_inserted = process_data_in_chunks(dataframe, chunk_size, CONSUMPTION_RANGES, n, get_range_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result_data_groups[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_consumption_groups(final_result_data_groups, LEVEL_SEPARATORS,max_groups=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_groups_to_csv(data_groups, input_csv_path):\n",
    "    # Construct the output directory and file path\n",
    "    base_name = os.path.splitext(os.path.basename(input_csv_path))[0]\n",
    "    output_dir = f\"./output/{base_name}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file_path = os.path.join(output_dir, 'data_groups.csv')\n",
    "    \n",
    "    # Prepare the data for saving\n",
    "    rows = []\n",
    "    for group_id, group in enumerate(data_groups, start=1):\n",
    "        for value, timestamp in group:\n",
    "            rows.append({\"Group_ID\": group_id, \"Value\": value, \"Timestamp\": timestamp})\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Data groups saved successfully to {output_file_path}\")\n",
    "\n",
    "save_data_groups_to_csv(final_result_data_groups, CSV_FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_first_n_groups_with_ranges_croped_pic(final_result_data_groups, CONSUMPTION_RANGES, n=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set formal font for the plot (e.g., Times New Roman)\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = 'Times New Roman'\n",
    "\n",
    "def save_vectorial_plot(groups, consumption_ranges, output_path, n=0, m=20):\n",
    "    # Use a seaborn color palette for better aesthetics\n",
    "    palette = sns.color_palette(\"husl\", m - n)  # Generate a harmonious color palette\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))  # Set figure size to 20 width by 10 height\n",
    "\n",
    "    # Initialize variables to determine y-axis limits\n",
    "    min_y = float('inf')\n",
    "    max_y = float('-inf')\n",
    "\n",
    "    # Plot the groups from n to m\n",
    "    for i, group in enumerate(groups[n:m], start=n):\n",
    "        group_indices = range(sum(len(g) for g in groups[:i]), sum(len(g) for g in groups[:i + 1]))\n",
    "        group_consumptions = [item[0] for item in group]\n",
    "\n",
    "        # Update y-axis limits\n",
    "        min_y = min(min_y, min(group_consumptions))\n",
    "        max_y = max(max_y, max(group_consumptions))\n",
    "\n",
    "        # Plot each group with a line and circles for data points\n",
    "        ax.plot(group_indices, group_consumptions, color=palette[i - n], linestyle='-')\n",
    "        ax.scatter(group_indices, group_consumptions, s=50, edgecolors='black', facecolors='none', marker='o', color=palette[i - n])\n",
    "\n",
    "    # Set y-axis limits to fit the data\n",
    "    buffer = (max_y - min_y) * 0.1  # Add some buffer space\n",
    "    ax.set_ylim(min_y - buffer, max_y + buffer)\n",
    "\n",
    "    # Plot horizontal lines for the consumption ranges and add text for the values\n",
    "    for i, (start, end) in enumerate(consumption_ranges):\n",
    "        line_color_start = \"blue\" if i % 2 == 0 else \"purple\"\n",
    "        line_color_end = \"green\" if i % 2 == 0 else \"orange\"\n",
    "        \n",
    "        # Add text just to the left of the horizontal lines, within the figure bounds\n",
    "        ax.text(x=ax.get_xlim()[0] + (buffer * 0.02)+170, y=end, s=f\"{end}\", color=line_color_end, va='center', ha='left', fontsize=20, bbox=dict(facecolor='white', edgecolor='none', alpha=1))\n",
    "        \n",
    "        ax.axhline(y=start, color=line_color_start, linestyle='-', linewidth=1.5)\n",
    "        ax.axhline(y=end, color=line_color_end, linestyle='-', linewidth=1.5)\n",
    "\n",
    "    ax.set_title('Time Series Data Grouped by Consumption Ranges with Range Lines', fontsize=20, pad=20)\n",
    "    ax.set_xlabel('Time Step', fontsize=20, labelpad=20)\n",
    "    ax.set_ylabel('Consumption', fontsize=20, labelpad=20)\n",
    "\n",
    "    ax.tick_params(axis='both', which='major', labelsize=20)  # Adjust the size of the tick labels\n",
    "    ax.tick_params(axis='both', which='both', labelcolor='black')  # Optional: Customize tick label color\n",
    "\n",
    "    # Save the plot as a vectorial PDF\n",
    "    plt.savefig(output_path, format='pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "save_vectorial_plot(final_result_data_groups, CONSUMPTION_RANGES, 'output_plot.pdf', n=0, m=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_result_data_groups)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "len(dataframe)\n",
    "\n",
    "\n",
    "root=None\n",
    "p=None\n",
    "leafTree=None\n",
    "consumptionLevel=None\n",
    "cntDataBotLevelTree=[]\n",
    "cluster=[]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  final_result_data_groups, total_inserted = process_data_in_chunks(dataframe, chunk_size, CONSUMPTION_RANGES, n, get_range_index)\n",
    "  for group in final_result_data_groups:\n",
    "    user_input,_ = group[0]\n",
    "    user_input = float(user_input)\n",
    "    consumptionLevel =get_consumption_level(user_input,CONSUMPTION_RANGES)\n",
    "    Insert (group,p,False)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time # Calculate the elapsed time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_root , total_count = sizeof_tree(root) , countDataPointInTree(root)\n",
    "var_count_total_node , var_count_artificial_node , var_count_real_node     =     countTotalNode(root), countArtificialNode(root) , countRealNode(root)\n",
    "\n",
    "print(\"__________________________________________________________\")\n",
    "print(\"Length of DataFrame: \",total_count , \"\\n Execution time:\", execution_time, \"seconds \\n Size of Tree :\", size_root, \"bytes  \\n Total Node = \", var_count_total_node)\n",
    "print(f\"Artificial Node = \",var_count_artificial_node,\" \\n Real Node = \",var_count_real_node, \"__________________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_result_data_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result_data_groups[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to split the static segments into chunks\n",
    "def static_segment_data(all_df, chunk_size):\n",
    "    # Convert DataFrame to NumPy array\n",
    "    numpy_array = all_df.to_numpy()\n",
    "    numpy_array = numpy_array[:, [1, 0]]  # Swap columns\n",
    "    \n",
    "    # Initialize list to store chunks\n",
    "    df_chunk = []\n",
    "\n",
    "    # Iterate and split into chunks\n",
    "    for i in range(0, len(numpy_array), chunk_size):\n",
    "        df_chunk.append(numpy_array[i:i + chunk_size])\n",
    "\n",
    "    return df_chunk\n",
    "vv=[]\n",
    "# Function to compute inertia metrics for the segments\n",
    "def compute_inertia_metrics(segments):\n",
    "    # Flatten the list to get all consumption values\n",
    "    all_values = np.array([consumption for segment in segments for consumption, _ in segment])\n",
    "\n",
    "    # Global center of gravity\n",
    "    G = np.mean(all_values)\n",
    "\n",
    "    # Total Inertia\n",
    "    I_total = np.mean((all_values - G) ** 2)\n",
    "    I_intra = 0\n",
    "    I_inter = 0\n",
    "    n = len(all_values)\n",
    "\n",
    "    # Compute intra-class and inter-class inertia\n",
    "    for segment in segments:\n",
    "        segment_values = np.array([consumption for consumption, _ in segment])\n",
    "        n_j = len(segment_values)\n",
    "\n",
    "        # Local center of gravity for the segment\n",
    "        g_j = np.mean(segment_values)\n",
    "\n",
    "        # Intra-class inertia: compactness within the segment\n",
    "        I_j = np.mean((segment_values - g_j) ** 2)\n",
    "        I_intra += (n_j / n) * I_j\n",
    "        global vv\n",
    "        if n not in vv:\n",
    "            vv.append(n)\n",
    "    \n",
    "        # Inter-class inertia: separation from the global center of gravity\n",
    "        I_inter += (n_j / n) * ((g_j - G) ** 2)\n",
    "\n",
    "    # Quality Score\n",
    "    quality_score = I_inter / I_total if I_total > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"I_total\": I_total,\n",
    "        \"I_intra\": I_intra,\n",
    "        \"I_inter\": I_inter,\n",
    "        \"quality_score\": quality_score\n",
    "    }\n",
    "\n",
    "# Function to create a result table from a list of metrics\n",
    "def create_result_table(metrics_list):\n",
    "    # Convert the list of metrics dictionaries into a DataFrame\n",
    "    df = pd.DataFrame(metrics_list)\n",
    "    \n",
    "    # Display the result table\n",
    "    return df\n",
    "\n",
    "# Example usage with your data\n",
    "if __name__ == \"__main__\":\n",
    "    # Compute metrics for final_result_data_groups\n",
    "    metrics_final_result = compute_inertia_metrics(final_result_data_groups)\n",
    "\n",
    "    # Compute metrics for static segments with different chunk sizes\n",
    "    chunk_sizes = [15,25,50]\n",
    "\n",
    "    metrics_static_chunks = []\n",
    "    \n",
    "    for chunk_size in chunk_sizes:\n",
    "        static_segments = static_segment_data(all_df, chunk_size)\n",
    "        metrics_static = compute_inertia_metrics(static_segments)\n",
    "        metrics_static_chunks.append(metrics_static)\n",
    "    \n",
    "    # Create result table\n",
    "    metrics_list = [metrics_final_result] + metrics_static_chunks\n",
    "    result_table = create_result_table(metrics_list)\n",
    "\n",
    "    # Add a column to specify chunk sizes for static segments\n",
    "    result_table['Chunk Size'] = [None] + chunk_sizes\n",
    "\n",
    "    # Print the result table\n",
    "    display(result_table)\n",
    "\n",
    "\n",
    "    base_name = os.path.splitext(os.path.basename(CSV_FILE_PATH))[0]\n",
    "    output_dir = f\"./output/{base_name}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file_path = os.path.join(output_dir, 'evaluation_segmentation.csv') \n",
    "    result_table=pd.DataFrame(result_table)\n",
    "    result_table.to_csv(output_file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv=[]\n",
    "metrics_final_result = compute_inertia_metrics(final_result_data_groups)\n",
    "vv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"background-color: #f5f5f5; color: #333; font-family: 'Helvetica Neue', Arial, sans-serif; font-size: 24px; text-align: center; padding: 20px 15px; border-radius: 8px; width: 80%; margin: 0 auto; font-weight: bold; text-shadow: 1px 1px 2px #ddd; border: 2px solid #333;\">5. Statistics and Evaluation</p>\n",
    "\n",
    "## 5.1 Calculating Statistics\n",
    "   - Code for calculating statistical measures (e.g., mean, median, variance).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def height_of_tree(node):\n",
    "  if node is None: return 0\n",
    "  else: return max(height_of_tree(node.left), height_of_tree(node.right)) + 1\n",
    "\n",
    "tree_height = height_of_tree(root)\n",
    "print(\"Height of the tree:\", tree_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"size of tree = \"+str( sizeof_tree(root)/ (1024 * 1024)) + \"megabytes\")\n",
    "print(\"size of data = \"+str( sys.getsizeof(final_result_data_groups)/ (1024 * 1024)) + \"megabytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_insertion_to_csv(\n",
    "    total_count, execution_time, size_root, \n",
    "    var_count_total_node, var_count_artificial_node, \n",
    "    var_count_real_node, tree_height, final_result_data_groups,\n",
    "    root, input_csv_path\n",
    "):\n",
    "    # Construct the output directory and file path\n",
    "    base_name = os.path.splitext(os.path.basename(input_csv_path))[0]\n",
    "    output_dir = f\"./output/{base_name}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file_path = os.path.join(output_dir, 'insertion.csv')\n",
    "    \n",
    "    # Calculate size of tree and data in megabytes\n",
    "    tree_size_mb = sizeof_tree(root) / (1024 * 1024)  # Convert bytes to MB\n",
    "    data_size_mb = sys.getsizeof(final_result_data_groups) / (1024 * 1024)  # Convert bytes to MB\n",
    "\n",
    "    # Prepare the data for saving\n",
    "    data = {\n",
    "        \"Length of DataFrame\": [total_count],\n",
    "        \"Execution Time (s)\": [execution_time],\n",
    "        \"Size of Tree (bytes)\": [size_root],\n",
    "        \"Total Nodes\": [var_count_total_node],\n",
    "        \"Artificial Nodes\": [var_count_artificial_node],\n",
    "        \"Real Nodes\": [var_count_real_node],\n",
    "        \"Tree Height\": [tree_height],\n",
    "        \"Size of Tree (MB)\": [tree_size_mb],\n",
    "        \"Size of Data (MB)\": [data_size_mb]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Insertion details saved successfully to {output_file_path}\")\n",
    "\n",
    "\n",
    "    # Print summary to console\n",
    "    print(\"__________________________________________________________\")\n",
    "    print(f\"Length of DataFrame: {total_count}\")\n",
    "    print(f\"Execution time: {execution_time} seconds\")\n",
    "    print(f\"Size of Tree: {size_root} bytes\")\n",
    "    print(f\"Total Node = {var_count_total_node}\")\n",
    "    print(f\"Artificial Node = {var_count_artificial_node}\")\n",
    "    print(f\"Real Node = {var_count_real_node}\")\n",
    "    print(f\"Height of the tree: {tree_height}\")\n",
    "    print(f\"Size of tree = {tree_size_mb} MB\")\n",
    "    print(f\"Size of data = {data_size_mb} MB\")\n",
    "    print(\"__________________________________________________________\")\n",
    "\n",
    "# insertion.csv\n",
    "\n",
    "save_insertion_to_csv(\n",
    "    total_count, execution_time, size_root, \n",
    "    var_count_total_node, var_count_artificial_node, \n",
    "    var_count_real_node, tree_height, final_result_data_groups, \n",
    "    root, CSV_FILE_PATH\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dictionary : { 2: 5 }  it means there are 5 tree nodes with lenght of 2 data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GU12NF9dn61G",
    "outputId": "ae3b5e0a-92a6-42c1-cf7e-e345710afc1a"
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "dictionnair = process_consumption_levels(root, len(LEVEL_SEPARATORS))  # Assuming `root` is defined elsewhere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "plot_schematic(dictionnair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "plot_schematic_ignore_small_values(dictionnair, threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionaries = {\n",
    "    1: dictionnair[0],\n",
    "    2: dictionnair[1],\n",
    "    3: dictionnair[2],\n",
    "    \"all\": dictionnair[3],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionaries['all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_dictionaries_to_csv(dictionaries, input_csv_path):\n",
    "    # Construct the output directory and file path\n",
    "    output_dir = f\"./output/{os.path.splitext(os.path.basename(input_csv_path))[0]}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file_path = os.path.join(output_dir, 'dictionaries.csv')\n",
    "\n",
    "    # Prepare data for saving: collect unique consumption values\n",
    "    consumption_values = set(dictionaries[1].keys()).union(dictionaries[2].keys(), dictionaries[3].keys(), dictionaries['all'].keys())\n",
    "\n",
    "    # Prepare the table data as a list of dictionaries\n",
    "    table_data = []\n",
    "\n",
    "    for consumption in consumption_values:\n",
    "        row = {\n",
    "            'consumption': consumption,\n",
    "            'count in level 1': dictionaries[1].get(consumption, 0),\n",
    "            'count in level 2': dictionaries[2].get(consumption, 0),\n",
    "            'count in level 3': dictionaries[3].get(consumption, 0),\n",
    "            'count in level all': dictionaries['all'].get(consumption, 0),\n",
    "        }\n",
    "        table_data.append(row)\n",
    "\n",
    "    # Convert the list of rows into a DataFrame\n",
    "    consumption_levels_df = pd.DataFrame(table_data)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    consumption_levels_df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Consumption levels saved successfully to {output_file_path}\")\n",
    "\n",
    "# dictionaries.csv\n",
    "# Save the data\n",
    "save_dictionaries_to_csv(dictionaries, CSV_FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "display_node_distribution(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming TreeNode and LeafNode classes are defined as you provided.\n",
    "\n",
    "def count_nodes_per_level(root):\n",
    "    if not root:\n",
    "        return {}\n",
    "\n",
    "    level_count = defaultdict(int)  # Dictionary to hold node counts per level\n",
    "    queue = deque([(root, 0)])  # Queue to hold (node, level)\n",
    "\n",
    "    while queue:\n",
    "        node, level = queue.popleft()\n",
    "        level_count[level] += 1  # Increment the count for the current level\n",
    "\n",
    "        # Traverse left and right children, if they exist\n",
    "        if node.left:\n",
    "            queue.append((node.left, level + 1))\n",
    "        if node.right:\n",
    "            queue.append((node.right, level + 1))\n",
    "\n",
    "    return dict(level_count)\n",
    "\n",
    "def plot_node_count_per_level_wide(node_count_per_level):\n",
    "    levels = list(node_count_per_level.keys())\n",
    "    counts = list(node_count_per_level.values())\n",
    "\n",
    "    plt.figure(figsize=(10, 5))  # Set the figure size to 25x5 inches\n",
    "    plt.bar(levels, counts)\n",
    "    plt.xlabel('Tree Level')\n",
    "    plt.ylabel('Number of Nodes')\n",
    "    plt.title('Node Count at Each Level')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_count_per_level = count_nodes_per_level(root)\n",
    "print(node_count_per_level)\n",
    "# plot_node_count_per_level_wide(node_count_per_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_node_count_per_level_to_csv(node_count_data, input_csv_path):\n",
    "    # Construct the output directory and file path\n",
    "    output_dir = f\"./output/{os.path.splitext(os.path.basename(input_csv_path))[0]}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file_path = os.path.join(output_dir, 'node_count_per_level.csv')\n",
    "\n",
    "    # Convert the dictionary into a DataFrame\n",
    "    node_count_df = pd.DataFrame(list(node_count_data.items()), columns=[\"Level\", \"Count Node\"])\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    node_count_df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Node count data saved successfully to {output_file_path}\")\n",
    "\n",
    "# node_count_per_level.csv\n",
    "save_node_count_per_level_to_csv(node_count_per_level, CSV_FILE_PATH)\n",
    "\n",
    "# =COUNTIF(B:B, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_result_table(node_count_per_level):\n",
    "    df = pd.DataFrame(list(node_count_per_level.items()), columns=['Level', 'Node Count'])\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "table = create_result_table(node_count_per_level)\n",
    "# display(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def generate_consumption_intervals(n, interval_type='small', percent_size=1.0, min_value=0, max_value=25691):\n",
    "    intervals = []\n",
    "    range_size = max_value - min_value\n",
    "    \n",
    "    # Define the percentage ranges for each type\n",
    "    interval_ranges = {\n",
    "        'small': (0.0, 0.2),\n",
    "        'mid': (0.2, 0.5),\n",
    "        'large': (0.5, 1.0)\n",
    "    }\n",
    "    \n",
    "    if interval_type not in interval_ranges:\n",
    "        raise ValueError(\"Invalid interval_type. Choose from 'small', 'mid', 'large'.\")\n",
    "\n",
    "    # Get the adjusted percentage range\n",
    "    start_percent, end_percent = interval_ranges[interval_type]\n",
    "    start_percent *= percent_size\n",
    "    end_percent *= percent_size\n",
    "\n",
    "    # Ensure percentages are within valid bounds\n",
    "    start_percent = max(0.0, start_percent)\n",
    "    end_percent = min(1.0, end_percent)\n",
    "\n",
    "    # Generate intervals\n",
    "    for _ in range(n):\n",
    "        interval_size = int(random.uniform(start_percent, end_percent) * range_size)  # Ensure integer size\n",
    "        min_interval = random.randint(int(min_value), int(max_value) - interval_size)  # Ensure integers\n",
    "        max_interval = min_interval + interval_size\n",
    "        intervals.append((min_interval, max_interval))\n",
    "\n",
    "    return intervals\n",
    "\n",
    "def find_smallest_and_largest_consumption_intervals(intervals):\n",
    "    # Initialize variables to store the smallest and largest consumption intervals\n",
    "    smallest_interval = None\n",
    "    largest_interval = None\n",
    "\n",
    "    smallest_duration = float('inf')\n",
    "    largest_duration = float('-inf')\n",
    "\n",
    "    for start, end in intervals:\n",
    "        duration = end - start\n",
    "        if duration < smallest_duration:\n",
    "            smallest_duration = duration\n",
    "            smallest_interval = (start, end)\n",
    "        if duration > largest_duration:\n",
    "            largest_duration = duration\n",
    "            largest_interval = (start, end)\n",
    "    \n",
    "    return smallest_interval, largest_interval\n",
    "\n",
    "def display_consumption_intervals_table(smallest_interval, largest_interval, intervals):\n",
    "    smallest_intersections = find_intersecting_indices(smallest_interval[0], smallest_interval[1], intervals)\n",
    "    largest_intersections = find_intersecting_indices(largest_interval[0], largest_interval[1], intervals)\n",
    "    \n",
    "    data = {\n",
    "        \"Interval Type\": [\"Smallest\", \"Largest\"],\n",
    "        \"Start Consumption\": [smallest_interval[0], largest_interval[0]],\n",
    "        \"End Consumption\": [smallest_interval[1], largest_interval[1]],\n",
    "        \"Duration (kW)\": [smallest_interval[1] - smallest_interval[0], largest_interval[1] - largest_interval[0]],\n",
    "        \"Intersecting Indices\": [smallest_intersections, largest_intersections]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    display(df)\n",
    "    \n",
    "def find_intersecting_indices(minConsumption, maxConsumption, CONSUMPTION_RANGES):\n",
    "    # Function to check intersection between two ranges\n",
    "    def check_intersection(range1, range2):\n",
    "        return max(range1[0], range2[0]) <= min(range1[1], range2[1])\n",
    "\n",
    "    # Find the indices of the ranges that intersect with the min and max consumption, starting index from 1\n",
    "    intersecting_indices = [\n",
    "        i + 1 for i, r in enumerate(CONSUMPTION_RANGES) if check_intersection([minConsumption, maxConsumption], r)\n",
    "    ]\n",
    "    \n",
    "    return intersecting_indices\n",
    "\n",
    "def generate_time_intervals(n, df):\n",
    "    start_time = df.iloc[0][TIME_COLUMN]\n",
    "    end_time = df.iloc[-1][TIME_COLUMN]\n",
    "\n",
    "    # Convert to datetime format\n",
    "    start_time = datetime.strptime(start_time, '%Y-%m-%d %H:%M')\n",
    "    end_time = datetime.strptime(end_time, '%Y-%m-%d %H:%M')\n",
    "\n",
    "    intervals = []\n",
    "\n",
    "    for _ in range(n):  # Generate n random time intervals\n",
    "        random_minutes = random.randint(0, int((end_time - start_time).total_seconds() / 60))  # Generate a random number of minutes to add to the start time\n",
    "        interval_start = start_time + timedelta(minutes=random_minutes)  # Calculate the start time of the interval\n",
    "        random_duration = random.randint(0, int((end_time - interval_start).total_seconds() / 60))  # Generate a random duration for the interval\n",
    "        interval_end = interval_start + timedelta(minutes=random_duration)  # Calculate the end time of the interval\n",
    "        \n",
    "        interval_start_str = interval_start.strftime('%Y-%m-%d %H:%M')\n",
    "        interval_end_str = interval_end.strftime('%Y-%m-%d %H:%M')\n",
    "        \n",
    "        # Append to the list of intervals\n",
    "        intervals.append((interval_start_str, interval_end_str))\n",
    "    \n",
    "    return intervals\n",
    "\n",
    "def find_smallest_and_largest_time_intervals(intervals):\n",
    "    # Initialize variables to store the smallest and largest time intervals\n",
    "    smallest_interval = None\n",
    "    largest_interval = None\n",
    "\n",
    "    smallest_duration = timedelta.max\n",
    "    largest_duration = timedelta.min\n",
    "\n",
    "    for start_str, end_str in intervals:\n",
    "        # Convert strings to datetime objects\n",
    "        start = datetime.strptime(start_str, '%Y-%m-%d %H:%M')\n",
    "        end = datetime.strptime(end_str, '%Y-%m-%d %H:%M')\n",
    "        duration = end - start\n",
    "        if duration < smallest_duration:\n",
    "            smallest_duration = duration\n",
    "            smallest_interval = (start, end)\n",
    "        if duration > largest_duration:\n",
    "            largest_duration = duration\n",
    "            largest_interval = (start, end)\n",
    "    \n",
    "    return smallest_interval, largest_interval\n",
    "\n",
    "def display_time_intervals_table(smallest_interval, largest_interval):\n",
    "    # Create a DataFrame to display the time intervals and their durations\n",
    "    data = {\n",
    "        \"Interval Type\": [\"Smallest\", \"Largest\"],\n",
    "        \"Start Time\": [smallest_interval[0].strftime('%Y-%m-%d %H:%M'), largest_interval[0].strftime('%Y-%m-%d %H:%M')],\n",
    "        \"End Time\": [smallest_interval[1].strftime('%Y-%m-%d %H:%M'), largest_interval[1].strftime('%Y-%m-%d %H:%M')],\n",
    "        \"Duration\": [str(smallest_interval[1] - smallest_interval[0]), str(largest_interval[1] - largest_interval[0])]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    print(df)\n",
    "    \n",
    "def generate_combined_intervals(n):\n",
    "    numeric_intervals = generate_consumption_intervals(n)\n",
    "    time_intervals = generate_time_intervals(n,df)\n",
    "    combined_intervals = list(zip(numeric_intervals, time_intervals))\n",
    "    \n",
    "    return combined_intervals\n",
    "\n",
    "# Display table for mixed time and consumption intervals\n",
    "def display_mixed_intervals_table(smallest_interval, largest_interval, intervals):\n",
    "    # Find intersecting indices based on both time and consumption\n",
    "    smallest_intersections = find_intersecting_indices_mixed_intervals(smallest_interval[2], smallest_interval[3], intervals)\n",
    "    largest_intersections = find_intersecting_indices_mixed_intervals(largest_interval[2], largest_interval[3], intervals)\n",
    "    \n",
    "    # Create a DataFrame with mixed data\n",
    "    data = {\n",
    "        \"Interval Type\": [\"Smallest\", \"Largest\"],\n",
    "        \"Start Time\": [smallest_interval[0].strftime('%Y-%m-%d %H:%M'), largest_interval[0].strftime('%Y-%m-%d %H:%M')],\n",
    "        \"End Time\": [smallest_interval[1].strftime('%Y-%m-%d %H:%M'), largest_interval[1].strftime('%Y-%m-%d %H:%M')],\n",
    "        \"Start Consumption (kW)\": [smallest_interval[2], largest_interval[2]],\n",
    "        \"End Consumption (kW)\": [smallest_interval[3], largest_interval[3]],\n",
    "        \"Total Duration (s)\": [(smallest_interval[1] - smallest_interval[0]).total_seconds() + (smallest_interval[3] - smallest_interval[2]),\n",
    "                               (largest_interval[1] - largest_interval[0]).total_seconds() + (largest_interval[3] - largest_interval[2])],\n",
    "        \"Intersecting Indices\": [smallest_intersections, largest_intersections]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    display(df)\n",
    "\n",
    "# Find intersecting indices for mixed time and consumption intervals\n",
    "def find_intersecting_indices_mixed_intervals(minConsumption, maxConsumption, intervals):\n",
    "    def check_intersection(range1, range2):\n",
    "        return max(range1[0], range2[0]) <= min(range1[1], range2[1])\n",
    "\n",
    "    # Find the indices of the ranges that intersect with the min and max consumption\n",
    "    intersecting_indices = [\n",
    "        i + 1 for i, r in enumerate([(c[2], c[3]) for c in intervals]) if check_intersection([minConsumption, maxConsumption], r)\n",
    "    ]\n",
    "    \n",
    "    return intersecting_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumption_intervals_small = generate_consumption_intervals(NUM_SEARCH_COUNT, interval_type='small', percent_size=1.0,min_value=0, max_value=df[\"Consumption\"].max())\n",
    "consumption_intervals_mid = generate_consumption_intervals(NUM_SEARCH_COUNT, interval_type='mid', percent_size=1.0,min_value=0, max_value=df[\"Consumption\"].max())\n",
    "consumption_intervals_large = generate_consumption_intervals(NUM_SEARCH_COUNT, interval_type='large', percent_size=1.0,min_value=0, max_value=df[\"Consumption\"].max())\n",
    "\n",
    "# print(\"Small intervals:\", consumption_intervals_small)\n",
    "# smallest_interval, largest_interval = find_smallest_and_largest_consumption_intervals(consumption_intervals_small)\n",
    "# display_consumption_intervals_table(smallest_interval, largest_interval,CONSUMPTION_RANGES)\n",
    "\n",
    "# print(\"Mid intervals:\", consumption_intervals_mid)\n",
    "# smallest_interval, largest_interval = find_smallest_and_largest_consumption_intervals(consumption_intervals_mid)\n",
    "# display_consumption_intervals_table(smallest_interval, largest_interval,CONSUMPTION_RANGES)\n",
    "\n",
    "# print(\"Large intervals:\", consumption_intervals_large)\n",
    "# smallest_interval, largest_interval = find_smallest_and_largest_consumption_intervals(consumption_intervals_large)\n",
    "# display_consumption_intervals_table(smallest_interval, largest_interval,CONSUMPTION_RANGES)\n",
    "\n",
    "consumption_intervals = {'small': consumption_intervals_small, \"mid\":consumption_intervals_mid, \"large\":consumption_intervals_large }\n",
    "display(consumption_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def save_consumption_intervals(consumption_intervals, output_dir, filename=\"consumption_intervals.csv\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file_path = os.path.join(output_dir, filename)\n",
    "    \n",
    "    # Flatten the data\n",
    "    rows = []\n",
    "    for interval_type, intervals in consumption_intervals.items():\n",
    "        for start, end in intervals:\n",
    "            rows.append({\n",
    "                'Interval Type': interval_type,\n",
    "                'Consumption Start': start,\n",
    "                'Consumption End': end\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame and save\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Consumption intervals saved to: {output_file_path}\")\n",
    "\n",
    "def load_consumption_intervals(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    consumption_intervals = {}\n",
    "    \n",
    "    # Reconstruct the dictionary\n",
    "    for _, row in df.iterrows():\n",
    "        interval_type = row['Interval Type']\n",
    "        start = row['Consumption Start']\n",
    "        end = row['Consumption End']\n",
    "        \n",
    "        if interval_type not in consumption_intervals:\n",
    "            consumption_intervals[interval_type] = []\n",
    "        \n",
    "        consumption_intervals[interval_type].append((start, end))\n",
    "    \n",
    "    print(f\"Consumption intervals loaded from: {file_path}\")\n",
    "    return consumption_intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_name = \"consumption_intervals_for_search.csv\"\n",
    "\n",
    "# Save consumption_intervals\n",
    "save_consumption_intervals(consumption_intervals, folder_name+\"/search intervals\", file_name)\n",
    "\n",
    "# Load consumption_intervals\n",
    "file_path = os.path.join(folder_name+\"/search intervals\", file_name)\n",
    "loaded_consumption_intervals = load_consumption_intervals(file_path)\n",
    "\n",
    "# Verify the result\n",
    "print(loaded_consumption_intervals)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "\n",
    "def generate_time_intervals(n, df):\n",
    "    start_time = datetime.strptime(df.iloc[0][TIME_COLUMN], '%Y-%m-%d %H:%M')\n",
    "    end_time = datetime.strptime(df.iloc[-1][TIME_COLUMN], '%Y-%m-%d %H:%M')\n",
    "    \n",
    "    # Total duration of the dataset in seconds\n",
    "    total_duration_seconds = (end_time - start_time).total_seconds()\n",
    "    \n",
    "    # Define the percentage ranges for each type\n",
    "    interval_ranges = {\n",
    "        'small': (0.0, 0.2),\n",
    "        'mid': (0.0, 0.6),\n",
    "        'large': (0.0, 1.0)\n",
    "    }\n",
    "    \n",
    "    # Define recency categories\n",
    "    recency_ranges = {\n",
    "        'new': (0.67, 1.0),       # Last 33% of the data\n",
    "        'little old': (0.33, .67),  # Last 67% of the data\n",
    "        'old': (0.0, .33)         # Entire data range\n",
    "    }\n",
    "    \n",
    "    # Initialize dictionary to store intervals\n",
    "    intervals = {itype: {recency: [] for recency in recency_ranges.keys()} for itype in interval_ranges.keys()}\n",
    "\n",
    "    for itype, (range_start, range_end) in interval_ranges.items():\n",
    "        for recency, (recency_start, recency_end) in recency_ranges.items():\n",
    "            for _ in range(n):\n",
    "                # Calculate interval duration range\n",
    "                min_duration = range_start * total_duration_seconds\n",
    "                max_duration = range_end * total_duration_seconds\n",
    "                \n",
    "                # Random interval duration\n",
    "                duration_seconds = random.uniform(min_duration, max_duration)\n",
    "                interval_duration = timedelta(seconds=duration_seconds)\n",
    "                \n",
    "                # Calculate the start time range for the recency type\n",
    "                recency_start_time = start_time + timedelta(seconds=recency_start * total_duration_seconds)\n",
    "                recency_end_time = start_time + timedelta(seconds=recency_end * total_duration_seconds)\n",
    "                \n",
    "                # Generate a random start time within the recency range\n",
    "                random_start_seconds = random.uniform(0, (recency_end_time - recency_start_time).total_seconds())\n",
    "                interval_start = recency_start_time + timedelta(seconds=random_start_seconds)\n",
    "                \n",
    "                # Calculate the end time\n",
    "                interval_end = min(interval_start + interval_duration, end_time)  # Ensure it doesn't go past the dataset\n",
    "                \n",
    "                # Append to the appropriate category\n",
    "                intervals[itype][recency].append({\n",
    "                    'start': interval_start.strftime('%Y-%m-%d %H:%M'),\n",
    "                    'end': interval_end.strftime('%Y-%m-%d %H:%M')\n",
    "                })\n",
    "    \n",
    "    return intervals\n",
    "\n",
    "def save_intervals_to_csv(intervals, input_csv_path):\n",
    "    \"\"\"\n",
    "    Saves the generated intervals to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    - intervals: A dictionary containing interval types and recency types with their intervals.\n",
    "    - input_csv_path: The path to the input CSV file to determine the output directory.\n",
    "    \"\"\"    \n",
    "    # Flatten the dictionary into a list of rows for the DataFrame\n",
    "    rows = []\n",
    "    for interval_type, recency_dict in intervals.items():\n",
    "        for recency_type, interval_list in recency_dict.items():\n",
    "            for interval in interval_list:\n",
    "                rows.append({\n",
    "                    'Interval Type': interval_type,\n",
    "                    'Recency Type': recency_type,\n",
    "                    'Start Time': interval['start'],\n",
    "                    'End Time': interval['end']\n",
    "                })\n",
    "    \n",
    "    # Create a DataFrame and save it to CSV\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(input_csv_path, index=False)\n",
    "    print(f\"Intervals saved to: {input_csv_path}\")\n",
    "\n",
    "def load_intervals_from_csv(input_csv_path):\n",
    "    \"\"\"\n",
    "    Loads intervals from a previously saved CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_csv_path: The path to the input CSV file to locate the saved intervals.\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary similar to the original intervals structure.\n",
    "    \"\"\"\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    \n",
    "    # Convert the DataFrame back into the nested dictionary structure\n",
    "    intervals = {}\n",
    "    for _, row in df.iterrows():\n",
    "        interval_type = row['Interval Type']\n",
    "        recency_type = row['Recency Type']\n",
    "        interval = {'start': row['Start Time'], 'end': row['End Time']}\n",
    "        \n",
    "        if interval_type not in intervals:\n",
    "            intervals[interval_type] = {}\n",
    "        if recency_type not in intervals[interval_type]:\n",
    "            intervals[interval_type][recency_type] = []\n",
    "        \n",
    "        intervals[interval_type][recency_type].append(interval)\n",
    "    \n",
    "    print(f\"Intervals loaded from: {input_csv_path}\")\n",
    "    return intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_intervals = generate_time_intervals(NUM_SEARCH_COUNT, df)\n",
    "\n",
    "display(time_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "save_intervals_to_csv(time_intervals, folder_name+'/search intervals/time_intervals_for_search.csv')\n",
    "\n",
    "# Load from CSV\n",
    "loaded_time_intervals = load_intervals_from_csv(folder_name+'/search intervals/time_intervals_for_search.csv')\n",
    "\n",
    "# Verify\n",
    "display(loaded_time_intervals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_intervals(consumption_intervals, time_intervals):\n",
    "    combined_intervals = {}\n",
    "    \n",
    "    for interval_type, consumption_list in consumption_intervals.items():\n",
    "        if interval_type not in time_intervals:\n",
    "            continue  # Skip if the interval type is missing in time_intervals\n",
    "        \n",
    "        combined_intervals[interval_type] = {}\n",
    "        recency_data = time_intervals[interval_type]\n",
    "        \n",
    "        for recency_type, time_list in recency_data.items():\n",
    "            # Pair consumption intervals with time intervals\n",
    "            combined_intervals[interval_type][recency_type] = []\n",
    "            for consumption, time in zip(consumption_list, time_list):\n",
    "                combined_intervals[interval_type][recency_type].append({\n",
    "                    'consumption': consumption,\n",
    "                    'time': time\n",
    "                })\n",
    "    \n",
    "    return combined_intervals\n",
    "\n",
    "combined_intervals = combine_intervals(loaded_consumption_intervals, loaded_time_intervals)\n",
    "\n",
    "display(combined_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_combined_intervals(combined_intervals, output_dir, filename=\"combined_intervals.csv\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file_path = os.path.join(output_dir, filename)\n",
    "    \n",
    "    # Flatten the data\n",
    "    rows = []\n",
    "    for interval_type, recency_data in combined_intervals.items():\n",
    "        for recency, intervals in recency_data.items():\n",
    "            for entry in intervals:\n",
    "                rows.append({\n",
    "                    'Interval Type': interval_type,\n",
    "                    'Recency': recency,\n",
    "                    'Consumption Start': entry['consumption'][0],\n",
    "                    'Consumption End': entry['consumption'][1],\n",
    "                    'Time Start': entry['time']['start'],\n",
    "                    'Time End': entry['time']['end']\n",
    "                })\n",
    "    \n",
    "    # Convert to DataFrame and save\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Combined intervals saved to: {output_file_path}\")\n",
    "\n",
    "\n",
    "def load_combined_intervals(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    combined_intervals = {}\n",
    "    \n",
    "    # Reconstruct the dictionary\n",
    "    for _, row in df.iterrows():\n",
    "        interval_type = row['Interval Type']\n",
    "        recency = row['Recency']\n",
    "        consumption = (row['Consumption Start'], row['Consumption End'])\n",
    "        time = {'start': row['Time Start'], 'end': row['Time End']}\n",
    "        \n",
    "        if interval_type not in combined_intervals:\n",
    "            combined_intervals[interval_type] = {}\n",
    "        \n",
    "        if recency not in combined_intervals[interval_type]:\n",
    "            combined_intervals[interval_type][recency] = []\n",
    "        \n",
    "        combined_intervals[interval_type][recency].append({\n",
    "            'consumption': consumption,\n",
    "            'time': time\n",
    "        })\n",
    "    \n",
    "    print(f\"Combined intervals loaded from: {file_path}\")\n",
    "    return combined_intervals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir,file_name = folder_name+\"/search intervals/\",\"combined_intervals_for_search.csv\"\n",
    "\n",
    "# Save combined_intervals\n",
    "save_combined_intervals(combined_intervals, output_dir, file_name)\n",
    "\n",
    "# Load combined_intervals\n",
    "file_path = os.path.join(output_dir, file_name)\n",
    "loaded_combined_intervals = load_combined_intervals(file_path)\n",
    "display(loaded_combined_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfFqGsqVvm3y",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Number_of_nodes_traversed=0\n",
    "Number_of_nodes_traversed+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IdM_w6N-4Zz0"
   },
   "outputs": [],
   "source": [
    "state=False\n",
    "def searchByTime(tree,start,end):\n",
    "  all_data =[]\n",
    "  if isinstance(tree,TreeNode):\n",
    "    if  end < tree.key :\n",
    "      if tree.left is not None:\n",
    "        all_data +=  searchByTime(tree.left,start,end)\n",
    "    elif  tree.key <= start:\n",
    "      if tree.right is not None:\n",
    "        all_data +=  searchByTime(tree.right,start,end)\n",
    "    else :\n",
    "      if tree.left is not None:\n",
    "        all_data +=  searchByTime(tree.left,start,end)\n",
    "      if tree.right is not None:\n",
    "        all_data +=  searchByTime(tree.right,start,end)\n",
    "\n",
    "  else:\n",
    "    if  end < tree.key :\n",
    "      if tree.left is not None:\n",
    "        all_data +=  searchByTime(tree.left,start,end)\n",
    "    elif  tree.key <= start:\n",
    "      if tree.timeRange[0]<=end and tree.timeRange[1]>=start :\n",
    "        all_data += subclusterByTime(tree.data, start, end)\n",
    "      if tree.right is not None and tree.timeRange[1] < end :\n",
    "        all_data +=  searchByTime(tree.right,start,end)\n",
    "    else :\n",
    "      if tree.left is not None:\n",
    "        all_data +=  searchByTime(tree.left,start,end)\n",
    "      if tree.timeRange[0]<=end and tree.timeRange[1]>=start :\n",
    "        all_data += subclusterByTime(tree.data, start, end)\n",
    "      if tree.right is not None:\n",
    "        all_data +=  searchByTime(tree.right,start,end)\n",
    "\n",
    "  return all_data\n",
    "\n",
    "def subclusterByTime(cluster, start, end):\n",
    "  table = []  # Initialize an empty list to store subclustered data points\n",
    "  for (data, time) in cluster: # Iterate through each data point in the cluster\n",
    "    if start <= time <= end: # Check if the data point's time falls within the specified range\n",
    "      table.append((data, time))  # If it does, add the data point to the subcluster\n",
    "  return table  # Return the subclustered data points within the specified time range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FWf5KDqrUfCv",
    "outputId": "81554ba4-de88-4276-eb27-3517944c5102"
   },
   "outputs": [],
   "source": [
    "\n",
    "# start_time = time.time()\n",
    "# printSearch=[]\n",
    "# for start_date, end_date in time_intervals:    # Loop through each pair of dates and call the search function\n",
    "#   printSearch.append(searchByTime(root,start_date, end_date))\n",
    "\n",
    "# end_time = time.time()\n",
    "# elapsed_time = end_time - start_time # Calculate the elapsed time\n",
    "\n",
    "# print(\"Elapsed time:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(printSearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - **5.3.2 Search by Consumption**\n",
    "      - Code and explanation for searching data by consumption.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFyyENd_p6s2"
   },
   "outputs": [],
   "source": [
    "state=False\n",
    "\n",
    "def searchByConsumptionManyLevel(tree,levels,minConsumption,maxConsumption):\n",
    "  all_data =[]\n",
    "  global state\n",
    "  if tree.key=='2015-01-01 2:15':\n",
    "    state=True\n",
    "  if state:\n",
    "    pdb.set_trace()\n",
    "  if tree.left is not None:\n",
    "    all_data +=  searchByConsumptionManyLevel(tree.left,levels,minConsumption,maxConsumption)\n",
    "  if isinstance(tree,LeafNode):\n",
    "    if tree.level in levels:\n",
    "      all_data+= subclusterByConsumption(tree.data,minConsumption,maxConsumption)\n",
    "  if tree.right is not None:\n",
    "    all_data +=  searchByConsumptionManyLevel(tree.right,levels,minConsumption,maxConsumption)\n",
    "  return all_data\n",
    "\n",
    "def subclusterByConsumption(cluster, min, max):  # Function to subcluster data points within a specified time range\n",
    "    table = []  # Initialize an empty list to store subclustered data points\n",
    "    for (data, time) in cluster: # Iterate through each data point in the cluster\n",
    "        if min <= data <= max:  # Check if the data point's time falls within the specified range\n",
    "            table.append((data, time))  # If it does, add the data point to the subcluster\n",
    "    return table  # Return the subclustered data points within the specified time range\n",
    "\n",
    "def searchByConsumptionOnLevel(tree,level,minConsumption,maxConsumption):\n",
    "  all_data =[]\n",
    "  if isinstance(tree,TreeNode):\n",
    "    if tree.left is not None:\n",
    "      all_data +=  searchByConsumptionOnLevel(tree.left,level,minConsumption,maxConsumption)\n",
    "    if tree.right is not None:\n",
    "      all_data +=  searchByConsumptionOnLevel(tree.right,level,minConsumption,maxConsumption)\n",
    "  else:\n",
    "    if tree.level > level :\n",
    "      if tree.left is not None:\n",
    "        all_data +=  searchByConsumptionOnLevel(tree.left,level,minConsumption,maxConsumption)\n",
    "      if tree.right is not None:\n",
    "        all_data +=  searchByConsumptionOnLevel(tree.right,level,minConsumption,maxConsumption)\n",
    "\n",
    "    if tree.level == level:\n",
    "      all_data+= subclusterByConsumption(tree.data,minConsumption,maxConsumption)\n",
    "  return all_data\n",
    "\n",
    "def searchByConsumption(tree,minConsumption,maxConsumption):\n",
    "    \n",
    "    levels=find_intersecting_indices(minConsumption, maxConsumption, CONSUMPTION_RANGES)\n",
    "    if len(levels)==1:\n",
    "        return searchByConsumptionOnLevel(tree,levels[0],minConsumption,maxConsumption)\n",
    "    else:\n",
    "        return searchByConsumptionManyLevel(tree,levels,minConsumption,maxConsumption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming `consumption_intervals_small`, `consumption_intervals_mid`, and `consumption_intervals_large` are defined\n",
    "consumption_intervals_sets = {\n",
    "    \"small\": consumption_intervals_small,\n",
    "    \"mid\": consumption_intervals_mid,\n",
    "    \"large\": consumption_intervals_large\n",
    "}\n",
    "print(f\"For each size of intervals, there are {len(consumption_intervals_sets)} intervals.\")\n",
    "\n",
    "# Iterate through each type of intervals\n",
    "for interval_type, intervals in consumption_intervals_sets.items():\n",
    "    start_time = time.time()\n",
    "    printSearch = []\n",
    "\n",
    "    for minconsumption, maxconsumption in intervals:  # Loop through each pair of intervals\n",
    "        printSearch.append(searchByConsumption(root, minconsumption, maxconsumption))\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_time = end_time - start_time  # Calculate the elapsed time\n",
    "    print(f\"Elapsed time for {interval_type} intervals:\", elapsed_time, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - **5.3.2 Search by Both Time and Consumption**\n",
    "      - Code and explanation for searching data by both time and consumption.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "naA3_tZqrCwo"
   },
   "outputs": [],
   "source": [
    "state=False\n",
    "def searchByTimeAndConsumptionsWithLevles(tree,start,end,levels,minConsumption,maxConsumption):\n",
    "  all_data =[]\n",
    "  if isinstance(tree,TreeNode):\n",
    "    if  end < tree.key :\n",
    "      if tree.left is not None:\n",
    "        all_data +=  searchByTimeAndConsumptionsWithLevles(tree.left,start,end,levels,minConsumption,maxConsumption)\n",
    "    elif  tree.key <= start:\n",
    "      if tree.right is not None:\n",
    "        all_data +=  searchByTimeAndConsumptionsWithLevles(tree.right,start,end,levels,minConsumption,maxConsumption)\n",
    "    else :\n",
    "      if tree.left is not None:\n",
    "        all_data +=  searchByTimeAndConsumptionsWithLevles(tree.left,start,end,levels,minConsumption,maxConsumption)\n",
    "      if tree.right is not None:\n",
    "        all_data +=  searchByTimeAndConsumptionsWithLevles(tree.right,start,end,levels,minConsumption,maxConsumption)\n",
    "\n",
    "  else:\n",
    "    if  end < tree.key :\n",
    "      if tree.left is not None:\n",
    "        all_data +=  searchByTimeAndConsumptionsWithLevles(tree.left,start,end,levels,minConsumption,maxConsumption)\n",
    "    elif  tree.key <= start:\n",
    "      if tree.timeRange[0]<=end and tree.timeRange[1]>=start  and tree.level in levels:\n",
    "        all_data += subclusterByTimeAndConsumptions(tree.data, start, end,minConsumption,maxConsumption)\n",
    "      if tree.right is not None and tree.timeRange[1] < end :\n",
    "        all_data +=  searchByTimeAndConsumptionsWithLevles(tree.right,start,end,levels,minConsumption,maxConsumption)\n",
    "    else :\n",
    "      if tree.left is not None:\n",
    "        all_data +=  searchByTimeAndConsumptionsWithLevles(tree.left,start,end,levels,minConsumption,maxConsumption)\n",
    "      if tree.timeRange[0]<=end and tree.timeRange[1]>=start and tree.level in levels:\n",
    "        all_data += subclusterByTimeAndConsumptions(tree.data, start, end,minConsumption,maxConsumption)\n",
    "      if tree.right is not None:\n",
    "        all_data +=  searchByTimeAndConsumptionsWithLevles(tree.right,start,end,levels,minConsumption,maxConsumption)\n",
    "\n",
    "  return all_data\n",
    "\n",
    "def subclusterByTimeAndConsumptions(cluster, start, end,minConsumption,maxConsumption):\n",
    "\n",
    "  table = []  # Initialize an empty list to store subclustered data points\n",
    "  for (data, time) in cluster: # Iterate through each data point in the cluster\n",
    "    if start <= time <= end and minConsumption <= data <= maxConsumption :\n",
    "      table.append((data, time))  # If it does, add the data point to the subcluster\n",
    "  return table  # Return the subclustered data points within the specified time range\n",
    "\n",
    "def searchByTimeAndConsumptions(tree,start,end,minConsumption,maxConsumption):\n",
    "    levels=find_intersecting_indices(minConsumption, maxConsumption, CONSUMPTION_RANGES)\n",
    "    return searchByTimeAndConsumptionsWithLevles(tree,start,end,levels,minConsumption,maxConsumption)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0kAFNaXm-BW"
   },
   "source": [
    "\n",
    "---\n",
    "# **Debugging**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QC0l9GrQTGQz",
    "outputId": "1d2289b5-833a-438c-ae12-164a4b221df9"
   },
   "outputs": [],
   "source": [
    "# vars(root.left.right)\n",
    "# sys.getsizeof(root.left.key)\n",
    "# print(time_range(root.right))\n",
    "# display(df)\n",
    "# allData(root)\n",
    "# print(len(allData(root)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CpUN53P3OpDL"
   },
   "outputs": [],
   "source": [
    "state=False\n",
    "\n",
    "def searchByTime(tree,start,end):\n",
    "  global axisNode,axisData,comparison\n",
    "\n",
    "  axisNode+=1\n",
    "  all_data =[]\n",
    "  comparison+=1\n",
    "  if isinstance(tree,TreeNode):\n",
    "    comparison+=2\n",
    "    if  end < tree.key :\n",
    "      comparison+=1-1\n",
    "      if tree.left is not None: all_data +=  searchByTime(tree.left,start,end)\n",
    "    elif  tree.key <= start:\n",
    "      comparison+=1\n",
    "      if tree.right is not None: all_data +=  searchByTime(tree.right,start,end)\n",
    "    else :\n",
    "      comparison+=1\n",
    "      if tree.left is not None: all_data +=  searchByTime(tree.left,start,end)\n",
    "      comparison+=1\n",
    "      if tree.right is not None: all_data +=  searchByTime(tree.right,start,end)\n",
    "  else:\n",
    "    comparison+=2\n",
    "    if  end < tree.key :\n",
    "      comparison+=1-1\n",
    "      if tree.left is not None: all_data +=  searchByTime(tree.left,start,end)\n",
    "    elif  tree.key <= start:\n",
    "      comparison+=1\n",
    "      if tree.timeRange[0]<=end and tree.timeRange[1]>=start :\n",
    "        axisData+=1\n",
    "        all_data += subclusterByTime(tree.data, start, end)\n",
    "      comparison+=1\n",
    "      if tree.right is not None and tree.timeRange[1] < end : all_data +=  searchByTime(tree.right,start,end)\n",
    "    else :\n",
    "      comparison+=1\n",
    "      if tree.left is not None: all_data +=  searchByTime(tree.left,start,end)\n",
    "      comparison+=1\n",
    "      if tree.timeRange[0]<=end and tree.timeRange[1]>=start :\n",
    "        axisData+=1\n",
    "        all_data += subclusterByTime(tree.data, start, end)\n",
    "      comparison+=1\n",
    "      if tree.right is not None: all_data +=  searchByTime(tree.right,start,end)\n",
    "  return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dXRSY2T8tjnQ",
    "outputId": "a733015e-e369-4065-ea2e-8453691ca14b"
   },
   "outputs": [],
   "source": [
    "output_file_path = os.path.join(folder_name, \"search time_intervals_results.csv\")\n",
    "\n",
    "# Prepare headers for the CSV file\n",
    "csv_headers = [\"Size\", \"Recency\", \"Total AxisNode\", \"Average AxisNode\",\n",
    "               \"Total AxisData\", \"Average AxisData\", \n",
    "               \"Total Comparison\", \"Average Comparison\", \n",
    "               \"Elapsed Time (seconds)\"]\n",
    "\n",
    "\n",
    "global search,axisData,comparison\n",
    "\n",
    "\n",
    "# Open the CSV file for writing\n",
    "with open(output_file_path, mode=\"w\", newline=\"\") as csvfile:\n",
    "  writer = csv.writer(csvfile)\n",
    "  writer.writerow(csv_headers)  # Write headers\n",
    "\n",
    "  for size,recency_data  in loaded_time_intervals.items():\n",
    "    print(\"\\n ---------------- in size : \",str(size),\"----------------\\n\")\n",
    "\n",
    "    for recency, intervals in recency_data.items():\n",
    "      print(\"\\n \\nin recency : \",str(recency),\"\\n -----------------------------------------------------------------------------\\n\")\n",
    "\n",
    "      axisData,axisNode,comparison =0,0,0\n",
    "\n",
    "      start_time = time.time()\n",
    "      printSearch=[]\n",
    "      total_axisNode,total_axisData,total_comparison = 0,0,0\n",
    "      for interval in intervals:     # Loop through each pair of dates and call the search function\n",
    "        start_date = interval['start']\n",
    "        end_date = interval['end']\n",
    "        \n",
    "        axisData,axisNode,comparison =0,0,0\n",
    "        searchByTime(root,start_date, end_date)\n",
    "        # print(\" axisNode:\", axisNode, \"\\n axisData:\", axisData, \"\\n comparison:\", comparison)\n",
    "        total_axisNode, total_axisData, total_comparison = total_axisNode + axisNode, total_axisData + axisData, total_comparison + comparison\n",
    "\n",
    "      average_axisNode, average_axisData, average_comparison = total_axisNode / len(intervals), total_axisData / len(intervals), total_comparison / len(intervals)\n",
    "\n",
    "      end_time = time.time()\n",
    "      elapsed_time = end_time - start_time # Calculate the elapsed time\n",
    "      print(\"Elapsed time:\", elapsed_time, \"seconds\")\n",
    "\n",
    "      print(\"Total axisNode:\", total_axisNode, \"Average axisNode:\", average_axisNode)\n",
    "      print(\"Total axisData:\", total_axisData, \"Average axisData:\", average_axisData)\n",
    "      print(\"Total comparison:\", total_comparison, \"Average comparison:\", average_comparison)\n",
    "\n",
    "      # Write results to the CSV file\n",
    "      writer.writerow([size, recency, total_axisNode, average_axisNode, \n",
    "                      total_axisData, average_axisData, \n",
    "                      total_comparison, average_comparison, \n",
    "                      elapsed_time])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ZkGvrvruMIa",
    "outputId": "b247ab40-e708-41c2-d981-69e4bca706dd"
   },
   "outputs": [],
   "source": [
    "size_root , total_count = sizeof_tree(root) , countDataPointInTree(root)\n",
    "var_count_total_node , var_count_artificial_node , var_count_real_node     =     countTotalNode(root), countArtificialNode(root) , countRealNode(root)\n",
    "\n",
    "print(\"__________________________________________________________\")\n",
    "print(\"Length of DataFrame: \",total_count , \"\\n Execution time:\", execution_time, \"seconds \\n Size of Tree :\", size_root, \"bytes  \\n Total Node = \", var_count_total_node)\n",
    "print(f\"Artificial Node = \",var_count_artificial_node,\" \\n Real Node = \",var_count_real_node, \"\\n__________________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OT-Ylx7TwWnm"
   },
   "outputs": [],
   "source": [
    "state=False\n",
    "\n",
    "def searchByConsumptionManyLevel(tree,levels,minConsumption,maxConsumption):\n",
    "  all_data =[]\n",
    "  global state,search,axisData,comparison\n",
    "  search+=1\n",
    "  comparison+=1\n",
    "  if tree.key=='2015-01-01 2:15': state=True\n",
    "  comparison+=1\n",
    "  if state: pdb.set_trace()\n",
    "  comparison+=1\n",
    "  if tree.left is not None: all_data +=  searchByConsumptionManyLevel(tree.left,levels,minConsumption,maxConsumption)\n",
    "  comparison+=1\n",
    "  if isinstance(tree,LeafNode):\n",
    "    comparison+=1\n",
    "    if tree.level in levels:\n",
    "      axisData+=1\n",
    "      all_data+= subclusterByConsumption(tree.data,minConsumption,maxConsumption)\n",
    "  comparison+=1\n",
    "  if tree.right is not None: all_data +=  searchByConsumptionManyLevel(tree.right,levels,minConsumption,maxConsumption)\n",
    "  return all_data\n",
    "\n",
    "def searchByConsumptionOnLevel(tree,level,minConsumption,maxConsumption):\n",
    "  all_data =[]\n",
    "  global search,axisData,comparison\n",
    "  search+=1\n",
    "  comparison+=1\n",
    "  if isinstance(tree,TreeNode):\n",
    "    comparison+=1\n",
    "    if tree.left is not None: all_data +=  searchByConsumptionOnLevel(tree.left,level,minConsumption,maxConsumption)\n",
    "    comparison+=1\n",
    "    if tree.right is not None: all_data +=  searchByConsumptionOnLevel(tree.right,level,minConsumption,maxConsumption)\n",
    "  else:\n",
    "    comparison+=1\n",
    "    if tree.level > level :\n",
    "      comparison+=1\n",
    "      if tree.left is not None: all_data +=  searchByConsumptionOnLevel(tree.left,level,minConsumption,maxConsumption)\n",
    "      comparison+=1\n",
    "      if tree.right is not None: all_data +=  searchByConsumptionOnLevel(tree.right,level,minConsumption,maxConsumption)\n",
    "    comparison+=1\n",
    "    if tree.level == level:\n",
    "      axisData+=1\n",
    "      all_data+= subclusterByConsumption(tree.data,minConsumption,maxConsumption)\n",
    "  return all_data\n",
    "\n",
    "\n",
    "# Find intersecting indices for consumption ranges with cost calculation\n",
    "def find_intersecting_indices_with_cost(minConsumption, maxConsumption, CONSUMPTION_RANGES):\n",
    "    # Function to check intersection between two ranges\n",
    "    \n",
    "    def check_intersection(range1, range2):\n",
    "        global search, axisData, comparison\n",
    "        comparison += 2\n",
    "        return max(range1[0], range2[0]) <= min(range1[1], range2[1])\n",
    "\n",
    "    # Find the indices of the ranges that intersect with the min and max consumption, starting index from 1\n",
    "    intersecting_indices = [\n",
    "        i + 1 for i, r in enumerate(CONSUMPTION_RANGES) if check_intersection([minConsumption, maxConsumption], r)\n",
    "    ]\n",
    "    \n",
    "    return intersecting_indices\n",
    "\n",
    "\n",
    "def searchByConsumption(tree,minConsumption,maxConsumption):\n",
    "    global search,axisData,comparison\n",
    "    comparison+=1\n",
    "    levels=find_intersecting_indices(minConsumption, maxConsumption, CONSUMPTION_RANGES)\n",
    "    if len(levels)==1:\n",
    "        # print(\"searchByConsumptionOnLevel\")\n",
    "        # print(levels)\n",
    "        return searchByConsumptionOnLevel(tree,levels[0],minConsumption,maxConsumption)\n",
    "    else:\n",
    "        # print(\"searchByConsumptionManyLevel\")\n",
    "        # print(levels)\n",
    "        return searchByConsumptionManyLevel(tree,levels,minConsumption,maxConsumption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumption_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "output_file_path = os.path.join(folder_name, \"search consumption_results.csv\")\n",
    "\n",
    "# Prepare headers for the CSV file\n",
    "csv_headers = [\"Size\", \"Total axisNode\", \"Average axisNode\", \n",
    "               \"Total AxisData\", \"Average AxisData\", \n",
    "               \"Total Comparison\", \"Average Comparison\", \n",
    "               \"Elapsed Time (seconds)\"]\n",
    "\n",
    "\n",
    "\n",
    "global search,axisData,comparison\n",
    "\n",
    "# Open the CSV file for writing\n",
    "with open(output_file_path, mode=\"w\", newline=\"\") as csvfile:\n",
    "  writer = csv.writer(csvfile)\n",
    "  writer.writerow(csv_headers)  # Write headers\n",
    "\n",
    "  for size,intervals in loaded_consumption_intervals.items():\n",
    "    print(\"\\n \\nin size : \",str(size),\"\\n -----------------------------------------------------------------------------\\n\")\n",
    "    start_time = time.time()\n",
    "    printSearch=[]\n",
    "    total_search,total_axisData,total_comparison = 0,0,0\n",
    "\n",
    "    for minconsumption, maxconsumption in intervals:    # Loop through each pair of dates and call the search function\n",
    "      axisData,search,comparison =0,0,0\n",
    "      result=searchByConsumption(root,minconsumption,maxconsumption)\n",
    "      print(\" axisNode:\", search, \"\\n axisData:\", axisData, \"\\n comparison:\", comparison)\n",
    "      total_search, total_axisData, total_comparison = total_search + search, total_axisData + axisData, total_comparison + comparison\n",
    "\n",
    "    average_axisNode, average_axisData, average_comparison = total_search / len(intervals), total_axisData / len(intervals), total_comparison / len(intervals)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time # Calculate the elapsed time\n",
    "    print(\"Elapsed time:\", elapsed_time, \"seconds\")\n",
    "\n",
    "    print(\"Total axisNode:\", total_search, \"Average axisNode:\", average_axisNode)\n",
    "    print(\"Total axisData:\", total_axisData, \"Average axisData:\", average_axisData)\n",
    "    print(\"Total comparison:\", total_comparison, \"Average comparison:\", average_comparison)\n",
    "\n",
    "\n",
    "    # Write results to the CSV file\n",
    "    writer.writerow([size, total_search, average_axisNode, \n",
    "                         total_axisData, average_axisData, \n",
    "                         total_comparison, average_comparison, \n",
    "                         elapsed_time])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P2-mzL_A0Gs0",
    "outputId": "e954a84f-9624-4e3b-8b43-c0aa9ddb930b"
   },
   "outputs": [],
   "source": [
    "# global search,axisData,comparison\n",
    "\n",
    "# axisData,search,comparison =0,0,0\n",
    "# result=searchByConsumption(root,14400,17520)\n",
    "\n",
    "# print(\" axisData:\", axisData, \"\\n search:\", search, \"\\n comparison:\", comparison)\n",
    "\n",
    "# axisData,search,comparison =0,0,0\n",
    "\n",
    "\n",
    "# searchByConsumptionOnLevel(root,2,2.4,3.1)\n",
    "# print(\" axisData:\", axisData, \"\\n search:\", search, \"\\n comparison:\", comparison)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ir4cSdrx-UMZ"
   },
   "outputs": [],
   "source": [
    "state=False\n",
    "def searchByTimeAndConsumptionsWithLevels(tree,start,end,levels,minConsumption,maxConsumption):\n",
    "  global axisNode,axisData,comparison\n",
    "  all_data =[]\n",
    "  axisNode+=1\n",
    "  comparison+=1\n",
    "  if isinstance(tree,TreeNode):\n",
    "    comparison+=2\n",
    "    if  end < tree.key :\n",
    "      comparison+=1-1\n",
    "      if tree.left is not None: all_data +=  searchByTimeAndConsumptionsWithLevels(tree.left,start,end,levels,minConsumption,maxConsumption)\n",
    "    elif  tree.key <= start:\n",
    "      comparison+=1\n",
    "      if tree.right is not None: all_data +=  searchByTimeAndConsumptionsWithLevels(tree.right,start,end,levels,minConsumption,maxConsumption)\n",
    "    else :\n",
    "      comparison+=2\n",
    "      if tree.left is not None: all_data +=  searchByTimeAndConsumptionsWithLevels(tree.left,start,end,levels,minConsumption,maxConsumption)\n",
    "      if tree.right is not None: all_data +=  searchByTimeAndConsumptionsWithLevels(tree.right,start,end,levels,minConsumption,maxConsumption)\n",
    "  else:\n",
    "    comparison+=2\n",
    "    if  end < tree.key :\n",
    "      comparison+=1-1\n",
    "      if tree.left is not None: all_data +=  searchByTimeAndConsumptionsWithLevels(tree.left,start,end,levels,minConsumption,maxConsumption)\n",
    "    elif  tree.key <= start:\n",
    "      comparison+=2\n",
    "      if tree.timeRange[0]<=end and tree.timeRange[1]>=start  and tree.level in levels:\n",
    "        axisData+=1\n",
    "        all_data += subclusterByTimeAndConsumptions(tree.data, start, end,minConsumption,maxConsumption)\n",
    "      if tree.right is not None and tree.timeRange[1] < end : all_data +=  searchByTimeAndConsumptionsWithLevels(tree.right,start,end,levels,minConsumption,maxConsumption)\n",
    "    else :\n",
    "      comparison+=3\n",
    "      if tree.left is not None: all_data +=  searchByTimeAndConsumptionsWithLevels(tree.left,start,end,levels,minConsumption,maxConsumption)\n",
    "      if tree.timeRange[0]<=end and tree.timeRange[1]>=start and tree.level in levels:\n",
    "        axisData+=1\n",
    "        comparison+=len(tree.data)\n",
    "        all_data += subclusterByTimeAndConsumptions(tree.data, start, end,minConsumption,maxConsumption)\n",
    "      if tree.right is not None: all_data +=  searchByTimeAndConsumptionsWithLevels(tree.right,start,end,levels,minConsumption,maxConsumption)\n",
    "  return all_data\n",
    "\n",
    "def searchByTimeAndConsumptions(tree,start,end,minConsumption,maxConsumption):\n",
    "  global axisNode,axisData,comparison\n",
    "  levels=find_intersecting_indices(minConsumption, maxConsumption, CONSUMPTION_RANGES)\n",
    "  # print(levels)\n",
    "  return searchByTimeAndConsumptionsWithLevels(tree,start,end,levels,minConsumption,maxConsumption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "output_file_path = os.path.join(folder_name, \"searchByTimeAndConsumptions result.csv\")\n",
    "\n",
    "# Prepare headers for the CSV file\n",
    "csv_headers = [\"Size\", \"Recency\", \"Total AxisNode\", \"Average AxisNode\", \n",
    "               \"Total AxisData\", \"Average AxisData\", \n",
    "               \"Total Comparison\", \"Average Comparison\", \n",
    "               \"Elapsed Time (seconds)\"]\n",
    "\n",
    "\n",
    "global axisNode,axisData,comparison\n",
    "\n",
    "with open(output_file_path, mode=\"w\", newline=\"\") as csvfile:\n",
    "  writer = csv.writer(csvfile)\n",
    "  writer.writerow(csv_headers)  # Write headers\n",
    "\n",
    "\n",
    "  for size, recency_data in loaded_combined_intervals.items():\n",
    "    print(\"\\n ---------------- in size : \",str(size),\"----------------------\\n\")\n",
    "    for recency, intervals in recency_data.items():\n",
    "      print(\"\\n \\nin recency : \",str(recency),\"\\n -----------------------------------------------------------------------------\\n\")\n",
    "\n",
    "      axisData,axisNode,comparison =0,0,0\n",
    "\n",
    "      start_time = time.time()\n",
    "      printSearch=[]\n",
    "      total_axisNode,total_axisData,total_comparison = 0,0,0\n",
    "      for interval in intervals:\n",
    "        minconsumption, maxconsumption = interval['consumption']\n",
    "        start_date, end_date = interval['time']['start'], interval['time']['end']\n",
    "        \n",
    "        axisData,axisNode,comparison =0,0,0\n",
    "        result=searchByTimeAndConsumptions(root,start_date, end_date,minconsumption, maxconsumption)\n",
    "        # print(\" axisNode:\", axisNode, \"\\n axisData:\", axisData, \"\\n comparison:\", comparison)\n",
    "        total_axisNode, total_axisData, total_comparison = total_axisNode + axisNode, total_axisData + axisData, total_comparison + comparison\n",
    "\n",
    "      average_axisNode, average_axisData, average_comparison = total_axisNode / len(intervals), total_axisData / len(intervals), total_comparison / len(intervals)\n",
    "\n",
    "      end_time = time.time()\n",
    "      elapsed_time = end_time - start_time # Calculate the elapsed time\n",
    "      print(\"Elapsed time:\", elapsed_time, \"seconds\")\n",
    "\n",
    "      print(\"Total axisNode:\", total_axisNode, \"Average axisNode:\", average_axisNode)\n",
    "      print(\"Total axisData:\", total_axisData, \"Average axisData:\", average_axisData)\n",
    "      print(\"Total comparison:\", total_comparison, \"Average comparison:\", average_comparison)\n",
    "      # Write results to the CSV file\n",
    "      writer.writerow([size, recency, total_axisNode, average_axisNode, \n",
    "                             total_axisData, average_axisData, \n",
    "                             total_comparison, average_comparison, \n",
    "                             elapsed_time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MS7WtzJ9K-63",
    "outputId": "591f11e2-89d9-4e76-ef17-c0e40305fbf0"
   },
   "outputs": [],
   "source": [
    "# print(get_total_size(root))\n",
    "# print(get_total_size(final_result_data_groups))\n",
    "# print(get_total_size(all_df)/ (1024 * 1024))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "end_mem = process.memory_info().rss / (1024 ** 2)  # in MB\n",
    "print(f\"Memory usage increased by: {end_mem - start_mem} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current and peak memory usage in bytes\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "\n",
    "# Convert bytes to MB\n",
    "current_mb = current / (1024 * 1024)\n",
    "peak_mb = peak / (1024 * 1024)\n",
    "\n",
    "# Display the memory usage in MB\n",
    "print(f\"Current memory usage: {current_mb:.2f} MB\")\n",
    "print(f\"Peak memory usage: {peak_mb:.2f} MB\")\n",
    "\n",
    "# Stopping the library\n",
    "tracemalloc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "dcr"
    ]
   },
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "# Load the notebook\n",
    "notebook_file = 'TL_Tree.ipynb'\n",
    "with open(notebook_file) as f:\n",
    "    notebook = nbformat.read(f, as_version=4)\n",
    "\n",
    "# # Count the number of lines\n",
    "# total_lines = 0\n",
    "# for cell in notebook.cells:\n",
    "#     if cell.cell_type == 'code':\n",
    "#         total_lines += len(cell.source.splitlines())\n",
    "\n",
    "# print(f\"Number of lines in code cells: {total_lines}\")\n",
    "\n",
    "\n",
    "\n",
    "code_lines = sum(len(cell['source'].split('\\n')) for cell in notebook['cells'] if cell['cell_type'] == 'code')\n",
    "markdown_lines = sum(len(cell['source'].split('\\n')) for cell in notebook['cells'] if cell['cell_type'] == 'markdown')\n",
    "\n",
    "total_lines = code_lines + markdown_lines\n",
    "\n",
    "print(\"Total lines in code cells:\", code_lines)\n",
    "print(\"Total lines in markdown cells:\", markdown_lines)\n",
    "print(\"Total lines in the notebook:\", total_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"__________________________________________________________\")\n",
    "print(\"Length of DataFrame: \",total_count , \"\\n Execution time:\", execution_time, \"seconds \\n Size of Tree :\", size_root, \"bytes  \\n Total Node = \", var_count_total_node)\n",
    "print(f\"Artificial Node = \",var_count_artificial_node,\" \\n Real No de = \",var_count_real_node, \"__________________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the file where results will be saved\n",
    "SAVE_FILE_PATH = \"general_results.csv\"\n",
    "\n",
    "def save_results_to_single_file(csv_file_path, var_count_real_node, var_count_artificial_node, var_count_total_node):\n",
    "    # Extract the base name of the CSV file (without extension) for unique identification\n",
    "    base_name = os.path.splitext(os.path.basename(csv_file_path))[0]\n",
    "    \n",
    "    # Create a dictionary with the results\n",
    "    result_dict = {\n",
    "        'dataset_name': [base_name],\n",
    "        'var_count_real_node': [var_count_real_node],\n",
    "        'var_count_artificial_node': [var_count_artificial_node],\n",
    "        'var_count_total_node': [var_count_total_node]\n",
    "    }\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    result_df = pd.DataFrame(result_dict)\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if os.path.exists(SAVE_FILE_PATH):\n",
    "        # Read the existing data\n",
    "        existing_df = pd.read_csv(SAVE_FILE_PATH)\n",
    "        \n",
    "        # Check if base_name already exists\n",
    "        if base_name in existing_df['dataset_name'].values:\n",
    "            # Update the existing row\n",
    "            existing_df.loc[existing_df['dataset_name'] == base_name, ['var_count_real_node', 'var_count_artificial_node', 'var_count_total_node']] = \\\n",
    "                result_df[['var_count_real_node', 'var_count_artificial_node', 'var_count_total_node']].values\n",
    "        else:\n",
    "            # Append the new row\n",
    "            existing_df = pd.concat([existing_df, result_df], ignore_index=True)\n",
    "        \n",
    "        # Save the updated DataFrame back to the CSV file\n",
    "        existing_df.to_csv(SAVE_FILE_PATH, index=False)\n",
    "    else:\n",
    "        # If the file doesn't exist, create it and write the DataFrame\n",
    "        result_df.to_csv(SAVE_FILE_PATH, index=False)\n",
    "    \n",
    "    print(f\"Results saved in: {SAVE_FILE_PATH}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results_to_single_file(CSV_FILE_PATH, var_count_real_node, var_count_artificial_node, var_count_total_node)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = len(dataframe)  # Define the chunk size\n",
    "number_of_data_inserted=0\n",
    "range_dict={}\n",
    "\n",
    "final_result_data_groups, total_inserted = process_data_in_chunks(dataframe, chunk_size, CONSUMPTION_RANGES, n, get_range_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_result_data_groups) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframeORG=all_df.copy()\n",
    "print(dataframeORG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframeORG=all_df.copy()\n",
    "print(dataframeORG)\n",
    "columns=['Size', 'Total Nodes', 'Artificial Nodes', 'Real Nodes',\"execution_time\"]\n",
    "\n",
    "# Create a DataFrame to store results\n",
    "results_df = pd.DataFrame(columns=columns)\n",
    "for size in range(10000, len(dataframeORG), 10000):\n",
    "    start_time = time.time()\n",
    "\n",
    "    dataframe=dataframeORG.copy()[:size]\n",
    "    chunk_size = len(dataframe)  # Define the chunk size\n",
    "    number_of_data_inserted=0\n",
    "    range_dict={}\n",
    "\n",
    "    final_result_data_groups, total_inserted = process_data_in_chunks(dataframe, chunk_size, CONSUMPTION_RANGES, 0, get_range_index)\n",
    "\n",
    "    print(\"final_result_data_groups\",len(final_result_data_groups))\n",
    "    root=None\n",
    "    p=None\n",
    "    leafTree=None\n",
    "    consumptionLevel=None\n",
    "    cntDataBotLevelTree=[]\n",
    "    cluster=[]\n",
    "\n",
    "    for group in final_result_data_groups:\n",
    "        user_input,_ = group[0]\n",
    "        user_input = float(user_input)\n",
    "        consumptionLevel =get_consumption_level(user_input,CONSUMPTION_RANGES)\n",
    "        Insert (group,p,False)\n",
    "\n",
    "\n",
    "\n",
    "    size_root , total_count = sizeof_tree(root) , countDataPointInTree(root)\n",
    "    var_count_total_node , var_count_artificial_node , var_count_real_node     =     countTotalNode(root), countArtificialNode(root) , countRealNode(root)\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time # Calculate the elapsed time\n",
    "\n",
    "    # Append results to DataFrame\n",
    "    print(\"__________________________________________________________\")\n",
    "    print(\"range of boucle:\", size)\n",
    "    print(\"countDataPointInTree\",total_count)\n",
    "    print(\"var_count_total_node\",var_count_total_node)\n",
    "    print(\"var_count_artificial_node\",var_count_artificial_node)\n",
    "    print(\"var_count_real_node\",var_count_real_node)\n",
    "    print(\"__________________________________________________________\")\n",
    "    results_df = pd.concat([\n",
    "        results_df, \n",
    "        pd.DataFrame([[size, var_count_total_node, var_count_artificial_node, var_count_real_node,execution_time]], columns=columns)\n",
    "    ], ignore_index=True)\n",
    "\n",
    "\n",
    "# size_based_node_counts_results\n",
    "\n",
    "# Save the results to a CSV file\n",
    "\n",
    "output_dir = f\"./output/{os.path.splitext(os.path.basename(CSV_FILE_PATH))[0]}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file_path = os.path.join(output_dir, 'size_based_node_counts_results.csv')\n",
    "\n",
    "results_df.to_csv(output_file_path, index=False)\n",
    "print(f\"size_based_node_counts_results saved successfully to {output_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(var_real)+list(var_artificial)+list(var_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "file_path = 'output/AEP_hourly/dictionaries.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the column names\n",
    "print(\"Columns in the CSV file:\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the first and last columns\n",
    "df = df.iloc[:, [0, -1]]\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(\"Updated DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Display the updated DataFrame columns\n",
    "print(\"Updated DataFrame columns:\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate the new dataset\n",
    "expanded_data = []\n",
    "for _, row in df.iterrows():\n",
    "    expanded_data.extend([row['consumption']] * row['count in level all'])\n",
    "\n",
    "# Create a new DataFrame with the expanded data\n",
    "expanded_df = pd.DataFrame({'consumption': expanded_data})\n",
    "\n",
    "# Display the result\n",
    "print(expanded_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df['count in level all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming expanded_df is already defined\n",
    "# Replace 'expanded_data.xlsx' with your desired file name\n",
    "\n",
    "# output_dir = f\"./output/{os.path.splitext(os.path.basename(CSV_FILE_PATH))[0]}\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "# output_file_path = os.path.join(output_dir, 'expanded_data.xlsx')\n",
    "output_file_path = 'output/AEP_hourly/expanded_data.xlsx'\n",
    "# Save the DataFrame to an Excel file\n",
    "expanded_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "# Confirm the save\n",
    "print(f\"DataFrame saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(root.right.right)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "8G-yUNcgCL8S",
    "rDyP0jLLp1v0",
    "7CQnX2PKruG5",
    "M0kAFNaXm-BW"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
